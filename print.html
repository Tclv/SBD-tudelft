<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Supercomputing for Big Data - Lab Manual</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction/index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="introduction/goal-of-this-lab.html"><strong aria-hidden="true">1.1.</strong> Goal of this lab</a></li><li class="chapter-item expanded "><a href="introduction/code-repositories.html"><strong aria-hidden="true">1.2.</strong> Code repositories</a></li><li class="chapter-item expanded "><a href="introduction/groups.html"><strong aria-hidden="true">1.3.</strong> Groups</a></li><li class="chapter-item expanded "><a href="introduction/aws.html"><strong aria-hidden="true">1.4.</strong> AWS</a></li><li class="chapter-item expanded "><a href="introduction/grading.html"><strong aria-hidden="true">1.5.</strong> Grading</a></li></ol></li><li class="chapter-item expanded "><a href="getting-started/index.html"><strong aria-hidden="true">2.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="getting-started/docker.html"><strong aria-hidden="true">2.1.</strong> Docker</a></li><li class="chapter-item expanded "><a href="getting-started/scala.html"><strong aria-hidden="true">2.2.</strong> Scala</a></li><li class="chapter-item expanded "><a href="getting-started/apache-spark/index.html"><strong aria-hidden="true">2.3.</strong> Apache Spark</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="getting-started/apache-spark/resilient-distributed-datasets.html"><strong aria-hidden="true">2.3.1.</strong> Resilient Distributed Datasets</a></li><li class="chapter-item expanded "><a href="getting-started/apache-spark/dataframe-and-dataset.html"><strong aria-hidden="true">2.3.2.</strong> Dataframe and Dataset</a></li><li class="chapter-item expanded "><a href="getting-started/apache-spark/packaging-your-application-using-sbt.html"><strong aria-hidden="true">2.3.3.</strong> Packaging your application using SBT</a></li></ol></li><li class="chapter-item expanded "><a href="getting-started/amazon-web-services.html"><strong aria-hidden="true">2.4.</strong> Amazon Web Services</a></li><li class="chapter-item expanded "><a href="getting-started/apache-kafka.html"><strong aria-hidden="true">2.5.</strong> Apache Kafka</a></li><li class="chapter-item expanded "><a href="getting-started/openstreetmap.html"><strong aria-hidden="true">2.6.</strong> OpenStreetMap</a></li><li class="chapter-item expanded "><a href="getting-started/alos.html"><strong aria-hidden="true">2.7.</strong> ALOS Global Digital Surface Model</a></li></ol></li><li class="chapter-item expanded "><a href="lab1/index.html"><strong aria-hidden="true">3.</strong> Lab 1</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab1/before-you-start.html"><strong aria-hidden="true">3.1.</strong> Before you start</a></li><li class="chapter-item expanded "><a href="lab1/assignment.html"><strong aria-hidden="true">3.2.</strong> Assignment</a></li><li class="chapter-item expanded "><a href="lab1/deliverables.html"><strong aria-hidden="true">3.3.</strong> Deliverables</a></li><li class="chapter-item expanded "><a href="lab1/rubric.html"><strong aria-hidden="true">3.4.</strong> Rubric</a></li></ol></li><li class="chapter-item expanded "><a href="lab2/index.html"><strong aria-hidden="true">4.</strong> Lab 2</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab2/before-you-start.html"><strong aria-hidden="true">4.1.</strong> Before you start</a></li><li class="chapter-item expanded "><a href="lab2/assignment.html"><strong aria-hidden="true">4.2.</strong> Assignment</a></li><li class="chapter-item expanded "><a href="lab2/deliverables.html"><strong aria-hidden="true">4.3.</strong> Deliverables</a></li><li class="chapter-item expanded "><a href="lab2/rubric.html"><strong aria-hidden="true">4.4.</strong> Rubric</a></li></ol></li><li class="chapter-item expanded "><a href="lab3/index.html"><strong aria-hidden="true">5.</strong> Lab 3</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab3/before-you-start.html"><strong aria-hidden="true">5.1.</strong> Before you start</a></li><li class="chapter-item expanded "><a href="lab3/assignment.html"><strong aria-hidden="true">5.2.</strong> Assignment</a></li><li class="chapter-item expanded "><a href="lab3/deliverables.html"><strong aria-hidden="true">5.3.</strong> Deliverables</a></li><li class="chapter-item expanded "><a href="lab3/rubric.html"><strong aria-hidden="true">5.4.</strong> Rubric</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><a href="faq.html"><strong aria-hidden="true">6.</strong> FAQ</a></li><li class="chapter-item expanded "><a href="quiz_example.html"><strong aria-hidden="true">7.</strong> Quiz example</a></li><li class="chapter-item expanded "><a href="links.html"><strong aria-hidden="true">8.</strong> Useful links</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">Supercomputing for Big Data - Lab Manual</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><a href="https://en.wikipedia.org/wiki/Sea_level_rise">Sea levels are rising</a>. If this continues, areas that are now land will become
part of the oceans. Potential inhabitants must relocate or live on the oceans,
just like Kevin Kostner and Jeanne Tripplehorn in <a href="https://www.imdb.com/title/tt0114898/">Waterworld (1995)</a>. In this
lab, we want to answer the question of what a good relocation plan would be for
inhabitants of flooded areas given a specific amount of sea level rise.</p>
<p>It turns out that we can achieve all the learning objectives of the course by
attempting to answer this question and some variants on it. To this end, we
limit ourselves to a context where we can only use the <a href="https://www.openstreetmap.org">OpenStreetMap</a>
and <a href="https://www.eorc.jaxa.jp/ALOS/en/aw3d30/index.htm">ALOS Global Digital Surface Model</a> datasets, together with various
industry-standard frameworks and tools, such as the <a href="https://www.scala-lang.org/">Scala programming language</a>
to write our programs, <a href="https://spark.apache.org">Apache Spark</a> to process batches of data, <a href="https://kafka.apache.org">Apache Kafka</a>
to process streaming data, and the <a href="https://aws.amazon.com">Amazon Web Services</a>
(AWS) to scale out our solutions in terms of performance (but also cost!). In
this context, we will face several challenges that are similar to what you could
find in the industry - and we will learn to deal with them efficiently.</p>
<h2 id="using-this-manual"><a class="header" href="#using-this-manual">Using this manual</a></h2>
<p>You can browse through the manual by using the table of contents on the left. To
go to the next page, you can also click on the <code>&gt;</code>'s on the right of this page.
This manual is generated by <a href="https://github.com/rust-lang/mdBook">mdBook</a>, and the sources can be found on <a href="https://github.com/abs-tudelft/sbd">GitHub</a>.
Feel free to kindly report issues and/or make pull requests to suggest or
implement improvements! If there are any major changes, we will notify everyone
on Brightspace and Discord.</p>
<h2 id="disclaimer"><a class="header" href="#disclaimer">Disclaimer</a></h2>
<p>The priority of this lab is to allow you to achieve all learning objectives of
the course. This becomes less boring and more effective if we do not choose the
assignments to be completely abstract and detached from reality (e.g. when we
ask you to join data set A with fields W and X to dataset B with fields Y and Z,
without any further context). The priority of this lab is not to teach you about
earth sciences or policymaking. The contents of this lab manual may include
oversimplifications of these subjects in order to allow us to achieve the
learning goals related to distributed computing on big data more effectively.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="goal-of-this-lab"><a class="header" href="#goal-of-this-lab">Goal of this Lab</a></h2>
<p>The goal of this lab is to achieve the Course Learning Objectives, that we 
repeat here.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<p>We will achieve these learning objectives by performing the following tasks,
spread out over three labs.</p>
<ul>
<li>You will work with Apache Spark, the MapReduce programming paradigm, and the 
Scala programming language, which is widely used in this domain.</li>
<li>You will approach a big data problem analytically and practically.</li>
<li>You will work with cloud-based systems.</li>
<li>You will deal with existing infrastructures for big data.</li>
<li>You will modify an existing application to operate in a streaming data 
context.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h3 id="code-repositories"><a class="header" href="#code-repositories">Code repositories</a></h3>
<p>For this lab, you will write a lot of code. All (incremental additions to) the
code for the lab assignments are kept in a private <a href="https://git-scm.com">git</a> repository that is
hosted on <a href="https://github.com">GitHub</a>, inside the <a href="https://classroom.github.com">GitHub Classroom</a> used for this course.
(Please don't register yet until you've read the full introduction.) This is the
<strong>only way</strong> to turn in code, any other channels, such as e-mail, are rejected.</p>
<p>If you do not know how to use git, you <em>must</em> familiarize yourself with it
first. This is an essential skill for your future career. We consider this a
prerequisite to the course. There are many tutorials online, and sometimes
<a href="https://rogerdudler.github.io/git-guide">cheatsheets</a> come in handy when you start to use git. There are also <a href="https://www.gitkraken.com">GUIs</a>
that may be useful to those that don't like working on the command-line.</p>
<p>The snapshot of the code in your assignment repository on the <code>master</code> branch,
at the moment the deadline expires, will be your submission. Make sure to make
your last commit before the deadline!</p>
<p>We recommend students to work on branches, and make pull requests to their
<code>master</code> branch once the code they are trying to implement is complete. These
pull requests can be reviewed by their teammates or by the TAs to provide early
feedback. </p>
<p>The feedback mechanism is also explicitly available in your GitHut Classroom
repository as a pull request that is made when your repository is first
initialized. To trigger the TA's to start reviewing your master branch, you can
also use this pull request (PRs). It will be named <code>Feedback #1</code> and can be
found by clicking <code>Pull requests</code>, then the <code>Feedback #1</code> PR. You can then ask
for a review from the TAs; ping TAs on Discord: <code>Yongding Tian</code>, <code>Vaatbak</code> or contact 
on GitHub <code>twoentartian</code>, <code>ChristiaanBoe</code>.
(create a new <code>Feedback</code> branch if it does not exist). 
Also see the figure below: </p>
<p><img src="introduction/../assets/images/review_request.png" alt="Where to find the pull requests and ask for review" /></p>
<p>Here is an example of feedback that we can provide based on the <code>Feedback #1</code> PR.
Note that you can also request reviews on any other PR if you want.</p>
<p><img src="introduction/../assets/images/review.png" alt="Example of a review requested through GitHub Classroom" /></p>
<p>Because we have limited capacity, we do ask you to request a review only when
you've made either very significant contributions, or preferably when you think
you have completed the assignment. TA priority will be based on groups ordered
by number of review requests performed (e.g. groups who have had only one
request will go before groups who have had several reviews).</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="groups"><a class="header" href="#groups">Groups</a></h2>
<p>You may work alone or in groups of at most two students. In the case of groups,
it is recommended that both students attempt to solve the problem before
converging to a single implementation.</p>
<p>In any case, even if you work alone, you must register your group in both GitHub
Classroom and Brightspace. We know this is inconvenient, but there is no
integration between Classroom and Brightspace yet. We use Brightspace for grade
administration, and GitHub Classroom for the code.</p>
<p>All groups in Classroom <strong>must</strong> be named after their respective name in
Brightspace, after the following example:</p>
<pre><code>2425-group-01
</code></pre>
<p>That is, <code>2425&lt;dash&gt;group&lt;dash&gt;&lt;two digit group number with leading zero&gt;</code>. If you do not
adhere to this group name format, some of our tools may not work, and we cannot
grade you.</p>
<p>The invite links for the GitHub Classrooms for the 3 assignments are as follows:</p>
<ul>
<li><a href="https://classroom.github.com/a/P8cPuKQI">Lab 1</a></li>
<li><a href="https://classroom.github.com/a/haYhCHb2">Lab 2</a></li>
<li><a href="https://classroom.github.com/a/3Z_hhj3Y">Lab 3</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h3 id="aws"><a class="header" href="#aws">AWS</a></h3>
<p>Lab 2 requires a significant amount of computations performed on a lot of data,
so we need a large compute and storage infrastructure to run the program. In
this lab, we will use AWS to facilitate this.</p>
<p>We have gotten AWS credits for the whole course via Cloud4Research, details will follow. (Disregard anything you heard about the GitHub student developer pack, it is no longer possible to get AWS credits that way.)</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="early-feedback-and-grading"><a class="header" href="#early-feedback-and-grading">Early Feedback and Grading</a></h2>
<p>Grading depends on the specific lab exercise and is explained in the associated
parts of the guide, but is generally based on:</p>
<ol>
<li>Your code</li>
<li>Your report</li>
</ol>
<p>Additionally, there is a potential oral exam based on the grade of a quiz at 
the end of the course (more about this at the end of this section).</p>
<p>All your code must be incrementally updated on GitHub Classroom. The reports are
also stored in your repository as the <code>README.md</code>. At some point, the deadline
will expire, and you will not be able to update your code and report. As 
explained previously, the code on the <code>master</code> branch will be graded.</p>
<p>We strive for a course where it is clear to you how you are doing before handing
in your project for grading, as well as making it clear how you are graded. In
this way, we do not have to rely on resits in our quest to achieve the learning
objectives, but will provide feedback much earlier, before handing in your
assignment for grading. Thus, this lab knows no resit (this is also only
required by the examination rules for written exams).</p>
<p>There are two mechanisms by which you can obtain <a href="https://en.wikipedia.org/wiki/Formative_assessment">early feedback</a> about your 
work.</p>
<p>a. Each lab will have an associated <a href="https://en.wikipedia.org/wiki/Rubric_(academic)">Rubric</a>, which is like a table with
indicators for specific grades in specific parts of the assignment, that are 
related to the learning objectives of this course.</p>
<p>b. You can ask the TAs during the lab for feedback on your code or your report, after you have looked at the rubric and/or if you have specific questions.</p>
<p>Therefore, by spending the required amount of time to learn, using the Rubric to
reflect on your own work, and by asking questions during the lab, it is incredibly unlikely that you will fail this course. At the same time, do remember that 
programming experience is a prerequisite for this course. We unfortunately do not 
have time to teach you how to program.</p>
<p>Regarding the oral exam, there is a possibility that some group members do not
contribute to achieving the lab goals, in turn causing them to not achieve the
learning objectives (sometimes called freeloaders). Because the TU Delft strives
to deliver Masters of Science of the highest quality, we must ensure that all
students that pass this course have achieved the learning objectives. To this
end, in one of the final weeks of the course, a multiple-choice quiz will take
place that is incredibly easy to pass if you achieved all learning goals through
the lab. If you do not pass (there will be a specific threshold), you are
invited to an oral examination where we will discuss the implementation of the
code. Based on this, we will determine a grade multiplier between zero and one.
Thus, make sure you understand every detail of the code your group may produce.
The quiz will be performed on Brightspace during a lecture slot. An example of
quiz questions is found <a href="introduction/../quiz_example.html">here</a>.</p>
<h3 id="regarding-plagiarism"><a class="header" href="#regarding-plagiarism">Regarding plagiarism</a></h3>
<p>All rules of the TU Delft and master program regarding plagiarism apply. It's
fine to copy little pieces of code that you understand from e.g. StackOverflow,
to build up your own desired functionality around it. It is definitely <strong>not</strong>
allowed to copy portions or the whole solution from other students. If you did
that, you wouldn't be achieving the learning objectives!</p>
<p>Additionally, <strong>DO NOT</strong> make your code publicly available by e.g. forking or
copying your repository publicly. The examination rules of the Delft University
of Technology explain that even though perhaps you wrote the code yourself,
making it publicly available makes you a potential accomplice to plagiarism and
you may receive the same punishment as whoever copied it from you. We repeat:
<em>DO NOT make your code publicly available</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>In this chapter, we will cover some of the concepts and technologies that are
used during the course.</p>
<h2 id="example-repository"><a class="header" href="#example-repository">Example repository</a></h2>
<p>The examples in this chapter are accompanied by some code. You can download this
code from its <a href="https://github.com/abs-tudelft/sbd-example">online repository</a>. Unless stated otherwise, we usually run
commands in the root folder of this repository. To get this code and go into the
root folder, you could run:</p>
<pre><code class="language-bash">git clone https://github.com/abs-tudelft/sbd-example.git
cd sbd-example
</code></pre>
<p>For command-line commands, we're going to assume we're using Linux with Bash. If
you're on Windows or Mac, you have to figure out how to do stuff yourself, or
perhaps use a virtual machine or container.</p>
<p>This chapter will continue to introduce the following topics:</p>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>An application that allows the user to package and run software (like Spark and
Kafka and the programs we write for them) in an isolated environment: a
container.</p>
<h2 id="scala"><a class="header" href="#scala">Scala</a></h2>
<p>A programming language that runs on the Java Virtual Machine (JVM). This is our
(mandatory!) language of choice during the lab assignments. We will use it to
program for both Apache Spark and Apache Kafka.</p>
<h2 id="apache-spark"><a class="header" href="#apache-spark">Apache Spark</a></h2>
<p>A framework for processing large amounts of data on multiple machines in a
robust way. We will build our application for labs 1 and 2 using Spark.</p>
<h2 id="amazon-web-services"><a class="header" href="#amazon-web-services">Amazon Web Services</a></h2>
<p>AWS, which provide theoretically unlimited compute infrastructure, allowing us
to process a large dataset in lab 2.</p>
<h2 id="apache-kafka"><a class="header" href="#apache-kafka">Apache Kafka</a></h2>
<p>A framework for building so-called data pipelines, in which potentially many
producers and consumers process real-time, streaming data. In lab 3, we will
take the application from labs 1 and 2 and modify it to process data in
real-time, using Kafka.</p>
<h2 id="openstreetmap"><a class="header" href="#openstreetmap">OpenStreetMap</a></h2>
<p>An open source project capturing geographic data from all over the world. The
assignments of this lab are based on (parts of) this data set.</p>
<h2 id="alos-global-digital-surface-model"><a class="header" href="#alos-global-digital-surface-model">ALOS Global Digital Surface Model</a></h2>
<p>An open data set with elevation levels of the entire planet (except oceans).</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="docker-1"><a class="header" href="#docker-1">Docker</a></h2>
<p>According to the <a href="https://docs.docker.com/get-started">Docker Documentation</a></p>
<blockquote>
<p>Docker is a platform for developers and sysadmins to develop, deploy, and run
applications with containers. The use of Linux containers to deploy
applications is called containerization. Containers are not new, but their
use for easily deploying applications is. Containerization is increasingly
popular because containers are:</p>
<p>Flexible</p>
<ul>
<li>Even the most complex applications can be containerized.</li>
</ul>
<p>Lightweight</p>
<ul>
<li>Containers leverage and share the host kernel.</li>
</ul>
<p>Interchangeable</p>
<ul>
<li>You can deploy updates and upgrades on-the-fly.</li>
</ul>
<p>Portable</p>
<ul>
<li>You can build locally, deploy to the cloud, and run anywhere.</li>
</ul>
<p>Scalable</p>
<ul>
<li>You can increase and automatically distribute container replicas.</li>
</ul>
<p>Stackable</p>
<ul>
<li>You can stack services vertically and on-the-fly.</li>
</ul>
</blockquote>
<p>For this course, we use Docker primarily to ensure every student is using the
exact same platform for their applications, and to avoid certain
platform-specific issues and peculiarities.</p>
<blockquote>
<p>You are <strong>not</strong> required to use Docker for this lab when you feel comfortable
setting up the required tools on your own system.</p>
</blockquote>
<p>A basic understanding of some <a href="https://docs.docker.com/">Docker</a> concepts helps
in getting started with this course. <a href="https://docs.docker.com/get-started/">Part 1: Orientation and
setup</a> of the <a href="https://docs.docker.com/get-started/">Get Started
Guide</a> covers the basic
<a href="https://docs.docker.com/">Docker</a> concepts used in this course.</p>
<p>Before trying the lab assignments and tutorials in the next sections, make sure
you <a href="https://docs.docker.com/install/#supported-platforms">Install Docker
(stable)</a> and test your
installation by running the simple <a href="https://hub.docker.com/_/hello-world">Hello World
image</a>.</p>
<pre><code class="language-bash">docker run hello-world
</code></pre>
<h3 id="setting-up-spark-in-docker"><a class="header" href="#setting-up-spark-in-docker">Setting up Spark in Docker</a></h3>
<p>In order to run Spark in a container, a <code>Dockerfile</code> is provided in the root of
all repositories we will use in this lab, including the repository for the
Getting Started guide. The <code>Dockerfile</code> can be used to build images for
<code>spark-submit</code> to run your Spark application, <code>spark-shell</code> to run a Spark
interactive shell, and the Spark history server to view event logs from
application runs. You need to build these images before you get started. The
Dockerfiles we provide assume that you run Docker from the folder at which they
are located. Don't move them around! They will stop working.</p>
<p>To build a docker image from the Dockerfile, we use <code>docker build</code>:</p>
<pre><code class="language-bash">docker build --target &lt;target&gt; -t &lt;tag&gt; .
</code></pre>
<p>Here <code>&lt;target&gt;</code> selects the target from the Dockerfile, <code>&lt;tag&gt;</code> sets the tag
for the resulting image, and the <code>.</code> sets the build context to the current
working directory.</p>
<p>We use <code>docker</code> to pull and build the images we need to use Spark and SBT.</p>
<ul>
<li>
<p><code>sbt</code></p>
<pre><code class="language-bash">docker pull hseeberger/scala-sbt:11.0.12_1.5.5_2.12.14
docker tag hseeberger/scala-sbt:11.0.12_1.5.5_2.12.14 sbt
</code></pre>
</li>
<li>
<p><code>spark-shell</code></p>
<pre><code class="language-bash">docker build --target spark-shell -t spark-shell .
</code></pre>
</li>
<li>
<p><code>spark-submit</code></p>
<pre><code class="language-bash">docker build --target spark-submit -t spark-submit .
</code></pre>
</li>
<li>
<p><code>spark-history-server</code></p>
<pre><code class="language-bash">docker build --target spark-history-server -t spark-history-server .
</code></pre>
</li>
</ul>
<p>You could then run the following commands from the Spark application root
(the folder containing the <code>build.sbt</code> file). Please make sure to use the
provided template project.</p>
<p>The commands below are provided as a reference, and they will be used throughout
the rest of this guide. You do not have to run them now, because some of them
require additional parameters (e.g. <code>spark-submit</code>) that we will provide later
in the manual.</p>
<ul>
<li>
<p>To run SBT to package or test your application (<code>sbt &lt;command&gt;</code>)</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/root sbt sbt
</code></pre>
</li>
<li>
<p>To start a Spark shell (<code>spark-shell</code>)</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io spark-shell
</code></pre>
</li>
<li>
<p>To run your Spark application (<code>spark-submit</code>) (fill in the class name of your
application and the name of your project!)</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io -v &quot;`pwd`&quot;/spark-events:/spark-events \
spark-submit --class &lt;YOUR_CLASSNAME&gt; \
target/scala-2.12/&lt;YOUR_PROJECT_NAME&gt;_2.12-1.0.jar
</code></pre>
</li>
<li>
<p>To spawn the history server to view event logs, accessible at
<a href="http://localhost:18080">localhost:18080</a></p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;/spark-events:/spark-events \
-p 18080:18080 spark-history-server
</code></pre>
</li>
</ul>
<p>The further we get in the manual, we will generally not mention the full Docker
commands this explicitly again, so know that if we mention e.g. <code>spark-shell</code>,
you should run the corresponding <code>docker run</code> command listed above. You can
create scripts or aliases for your favorite shell to avoid having to type a lot.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="scala-1"><a class="header" href="#scala-1">Scala</a></h2>
<p>Apache Spark, our big data framework of choice for this lab, is implemented in
Scala, a compiled language on the JVM that supports a mix between functional
and object-oriented programming. It is compatible with Java libraries. Some
reasons why Spark was written in Scala are:</p>
<ol>
<li>
<p>Compiling to the JVM makes the codebase extremely portable and deploying
applications as easy as sending the Java bytecode (typically packaged in a
<strong>J</strong>ava <strong>AR</strong>chive format, or JAR). This simplifies deploying to cloud
provider big data platforms as we don't need specific knowledge of the
operating system, or even the underlying architecture.</p>
</li>
<li>
<p>Compared to Java, Scala has some advantages in supporting more complex
types, type inference, and anonymous functions (Since Java 8, Java also supports anonymous functions, or
lambda expression, but this version wasn't released at the time of Spark's
initial release.). Matei
Zaharia, Apache Spark's original author, has said the following about why
Spark was implemented in Scala in a <a href="https://www.reddit.com/r/IAmA/comments/31bkue/im_matei_zaharia_creator_of_spark_and_cto_at/">Reddit AMA</a>:</p>
<blockquote>
<p>At the time we started, I really wanted a PL that supports a
language-integrated interface (where people write functions inline, etc),
because I thought that was the way people would want to program these
applications after seeing research systems that had it (specifically
Microsoft's DryadLINQ). However, I also wanted to be on the JVM in order to
easily interact with the Hadoop filesystem and data formats for that. Scala
was the only somewhat popular JVM language then that offered this kind of
functional syntax and was also statically typed (letting us have some control
over performance), so we chose that. Today there might be an argument to make
the first version of the API in Java with Java 8, but we also benefitted from
other aspects of Scala in Spark, like type inference, pattern matching, actor
libraries, etc.</p>
</blockquote>
</li>
</ol>
<p>Apache Spark provides interfaces to Scala, R, Java and Python, but we will be
using Scala to program in this lab. An introduction to Scala can be found on
the <a href="https://docs.scala-lang.org/tour/tour-of-scala.html">Scala language site</a>.
You can have a brief look at it, but you can also pick up topics as you go through the lab.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="apache-spark-1"><a class="header" href="#apache-spark-1">Apache Spark</a></h2>
<p>Apache Spark provides a programming model for a resilient distributed
shared memory model. To elaborate on this, Spark allows you to program against
a <em>unified view</em> of memory (i.e. RDD or DataFrame), while the processing
happens <em>distributed</em> over <em>multiple nodes/machines/computers/servers</em> being
able to compensate for <em>failures of these nodes</em>.</p>
<p>This allows us to define a computation and scale this over multiple machines
without having to think about communication, distribution of data, and
potential failures of nodes. This advantage comes at a cost: All applications
have to comply with Spark's (restricted) programming model.</p>
<p>The programming model Spark exposes is based around the MapReduce paradigm.
This is an important consideration when you would consider using Spark, does my
problem fit into this paradigm?</p>
<p>Modern Spark exposes two APIs around this programming model:</p>
<ol>
<li>Resilient Distributed Datasets</li>
<li>Spark SQL Dataframe/Datasets</li>
</ol>
<p>In the rest of this section, we will demonstrate a simple application with
implementations using both APIs.</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="resilient-distributed-datasets"><a class="header" href="#resilient-distributed-datasets">Resilient Distributed Datasets</a></h3>
<p><img src="getting-started/apache-spark/../../assets/images/RDD.png" alt="Illustration of RDD abstraction of an RDD with a tuple of characters and integers as elements." /></p>
<blockquote>
<p>Illustration of RDD abstraction of an RDD with a tuple of characters and integers as elements.</p>
</blockquote>
<p>RDDs are the original data abstraction used in Spark. Conceptually one can
think of these as a large, unordered list of Java/Scala/Python objects, let's
call these objects elements. This list of elements is divided in partitions
(which may still contain multiple elements), which can reside on different
machines. One can operate on these elements with a number of operations, which
can be subdivided in wide and narrow dependencies, see the table below. An
illustration of the RDD abstraction can be seen in the figure above.</p>
<p>RDDs are immutable, which means that the elements cannot be altered, without
creating a new RDD. Furthermore, the application of transformations (wide or
narrow) is <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation</a>,
meaning that the actual computation will be delayed until results are requested
(an action in Spark terminology). When applying transformations, these will
form a directed acyclic graph (DAG), that instructs workers what operations to
perform, on which elements to find a specific result. This can be seen in the
figure above as the arrows between elements.</p>
<table><thead><tr><th align="left">Narrow Dependency</th><th align="left">Wide Dependency</th></tr></thead><tbody>
<tr><td align="left"><code>map</code></td><td align="left"><code>coGroup</code></td></tr>
<tr><td align="left"><code>mapValues</code></td><td align="left"><code>flatMap</code></td></tr>
<tr><td align="left"><code>flatMap</code></td><td align="left"><code>groupByKey</code></td></tr>
<tr><td align="left"><code>filter</code></td><td align="left"><code>reduceByKey</code></td></tr>
<tr><td align="left"><code>mapPartitions</code></td><td align="left"><code>combineByKey</code></td></tr>
<tr><td align="left"><code>mapPartitionsWithIndex</code></td><td align="left"><code>distinct</code></td></tr>
<tr><td align="left"><code>join</code> with sorted keys</td><td align="left"><code>join</code></td></tr>
<tr><td align="left"></td><td align="left"><code>intersection</code></td></tr>
<tr><td align="left"></td><td align="left"><code>repartition</code></td></tr>
<tr><td align="left"></td><td align="left"><code>coalesce</code></td></tr>
<tr><td align="left"></td><td align="left"><code>sort</code></td></tr>
</tbody></table>
<blockquote>
<p>List of wide and narrow dependencies for (pair) RDD operations</p>
</blockquote>
<p>Now that you have an idea of what the abstraction is about, let's demonstrate
some example code with the Spark shell. </p>
<p><em>If you want to paste pieces of code into the spark shell from this guide, it
might be useful to copy from the github version, and use the <code>:paste</code> command in
the spark shell to paste the code. Hit <code>ctrl+D</code> to stop pasting.</em></p>
<p>We can start the shell using <a href="getting-started/apache-spark/../docker.html">Docker</a>:</p>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io spark-shell
</code></pre>
<p>We should now get the following output:</p>
<pre><code class="language-scala">WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2021-09-06 08:36:27,947 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://a4a64e0fea2c:4040
Spark context available as 'sc' (master = local[*], app id = local-1630917390997).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.12)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; 
</code></pre>
<p>When opening a Spark Shell, by default, you get two objects.</p>
<ul>
<li>A <code>SparkSession</code> named <code>spark</code>.</li>
<li>A <code>SparkContext</code> named <code>sc</code>. </li>
</ul>
<p>These objects contains the configuration of your session, i.e. whether you are
running in local or cluster mode, the name of your application, the logging
level etc.</p>
<p>We can get some (sometimes slightly arcane) information about Scala objects that
exist in the scope of the shell, e.g.:</p>
<pre><code class="language-scala">scala&gt; spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@747e0a31
</code></pre>
<p>Now, let's first create some sample data that we can demonstrate the RDD API
around. Here we create an infinite list of repeating characters from 'a' tot
'z'.</p>
<pre><code class="language-scala">scala&gt; val charsOnce = ('a' to 'z').toStream
charsOnce: scala.collection.immutable.Stream[Char] = Stream(a, ?)

scala&gt; val chars: Stream[Char] = charsOnce #::: chars
chars: Stream[Char] = Stream(a, ?)
</code></pre>
<p>Now we build a collection with the first 200000 integers, zipped with the
character stream. We display the first five results.</p>
<pre><code class="language-scala">scala&gt; val rdd = sc.parallelize(chars.zip(1 to 200000), numSlices=20)
rdd: org.apache.spark.rdd.RDD[(Char, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:26

scala&gt; rdd.take(5)
res2: Array[(Char, Int)] = Array((a,1), (b,2), (c,3), (d,4), (e,5))
</code></pre>
<p>Let's dissect what just happened. We created a Scala object that is a list of
tuples of <code>Char</code>s and <code>Int</code>s in the statement <code>(chars).zip(1 to 200000)</code>. With
<code>sc.parallelize</code> we are transforming a Scala sequence into an RDD. This allows
us to enter Spark's programming model. With the optional parameter <code>numSlices</code>
we indicate in how many partitions we want to subdivide the sequence.</p>
<p>Let's apply some (lazily evaluated) transformations to this RDD.</p>
<pre><code class="language-scala">scala&gt; val mappedRDD = rdd.map({case (chr, num) =&gt; (chr, num+1)})
mappedRDD: org.apache.spark.rdd.RDD[(Char, Int)] = MapPartitionsRDD[1] at map at &lt;console&gt;:25
</code></pre>
<p>We apply a <code>map</code> to the RDD, applying a function to all the elements in the
RDD. The function we apply pattern matches over the elements as being a tuple
of <code>(Char, Int)</code>, and add one to the integer. Scala's syntax can be a bit
foreign, so if this is confusing, spend some time looking at tutorials and
messing around in the Scala interpreter.</p>
<p>You might have noticed that the transformation completed awfully fast. This is
Spark's <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation</a> in
action. No computation will be performed until an action is applied.</p>
<pre><code class="language-scala">scala&gt; val reducedRDD = rdd.reduceByKey(_ + _)
reducedRDD: org.apache.spark.rdd.RDD[(Char, Int)] = ShuffledRDD[2] at reduceByKey at &lt;console&gt;:25
</code></pre>
<p>Now we apply a <code>reduceByKey</code> operation, grouping all of the identical keys together and
merging the results with the specified function, in this case the <code>+</code> operator.</p>
<p>Now we will perform an action, which will trigger the computation of the
transformations on the data. We will use the collect action, which means to
gather all the results to the master, going out of the Spark programming model,
back to a Scala sequence. How many elements do you expect there to be in this
sequence after the previous transformations?</p>
<pre><code class="language-scala">scala&gt; reducedRDD.collect
res3: Array[(Char, Int)] = Array((d,769300000), (x,769253844), (e,769307693),
(y,769261536), (z,769269228), (f,769315386), (g,769323079), (h,769330772),
(i,769138464), (j,769146156), (k,769153848), (l,769161540), (m,769169232),
(n,769176924), (o,769184616), (p,769192308), (q,769200000), (r,769207692),
(s,769215384), (t,769223076), (a,769276921), (u,769230768), (b,769284614),
(v,769238460), (w,769246152), (c,769292307))
</code></pre>
<p>Typically, we don't build the data first, but we actually load it from a
database or file system. Say we have some data in (multiple) files in a
specific format. As an example consider <code>sensordata.csv</code> (in the <code>example</code>
folder). We can load it as follows:</p>
<pre><code class="language-scala">// sc.textFile can take multiple files as argument!
scala&gt; val raw_data = sc.textFile(&quot;sensordata.csv&quot;)
raw_data: org.apache.spark.rdd.RDD[String] = sensordata.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:24
</code></pre>
<p>And observe some of its contents:</p>
<pre><code class="language-scala">scala&gt; raw_data.take(10).foreach(println)
COHUTTA,3/10/14:1:01,10.27,1.73,881,1.56,85,1.94
COHUTTA,3/10/14:1:02,9.67,1.731,882,0.52,87,1.79
COHUTTA,3/10/14:1:03,10.47,1.732,882,1.7,92,0.66
COHUTTA,3/10/14:1:05,9.56,1.734,883,1.35,99,0.68
COHUTTA,3/10/14:1:06,9.74,1.736,884,1.27,92,0.73
COHUTTA,3/10/14:1:08,10.44,1.737,885,1.34,93,1.54
COHUTTA,3/10/14:1:09,9.83,1.738,885,0.06,76,1.44
COHUTTA,3/10/14:1:11,10.49,1.739,886,1.51,81,1.83
COHUTTA,3/10/14:1:12,9.79,1.739,886,1.74,82,1.91
COHUTTA,3/10/14:1:13,10.02,1.739,886,1.24,86,1.79
</code></pre>
<p>We can process this data to filter only measurements on <code>3/10/14:1:01</code>.</p>
<pre><code class="language-scala">scala&gt; val filterRDD = raw_data.map(_.split(&quot;,&quot;)).filter(x =&gt; x(1) == &quot;3/10/14:1:01&quot;)
filterRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[11] at filter at &lt;console&gt;:25
</code></pre>
<p>And look at the output:</p>
<pre><code class="language-scala">scala&gt; filterRDD.foreach(a =&gt; println(a.mkString(&quot; &quot;)))
COHUTTA 3/10/14:1:01 10.27 1.73 881 1.56 85 1.94
LAGNAPPE 3/10/14:1:01 9.59 1.602 777 0.09 88 1.78
NANTAHALLA 3/10/14:1:01 10.47 1.712 778 1.96 76 0.78
CHER 3/10/14:1:01 10.17 1.653 777 1.89 96 1.57
THERMALITO 3/10/14:1:01 10.24 1.75 777 1.25 80 0.89
ANDOUILLE 3/10/14:1:01 10.26 1.048 777 1.88 94 1.66
BUTTE 3/10/14:1:01 10.12 1.379 777 1.58 83 0.67
CARGO 3/10/14:1:01 9.93 1.903 778 0.55 76 1.44
MOJO 3/10/14:1:01 10.47 1.828 967 0.36 77 1.75
BBKING 3/10/14:1:01 10.03 0.839 967 1.17 80 1.28
</code></pre>
<p>You might have noticed that this is a bit tedious to work with, as we have to
convert everything to Scala objects, and aggregations rely on having a pair RDD,
which is fine when we have a single key. For more complex aggregations, this
becomes a bit tedious to juggle with.</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="dataframe-and-dataset"><a class="header" href="#dataframe-and-dataset">Dataframe and Dataset</a></h3>
<p>Our previous example is quite a typical use case for Spark. We have a big data
store of some structured (tabular) format (be it csv, JSON, parquet, or
something else) that we would like to analyse, typically in some SQL-like
fashion. Manually applying operations to rows like this is both labour
intensive, and inefficient, as we have knowledge of the 'schema' of data. This
is where DataFrames originate from. Spark has an optimized SQL query engine that
can optimize the compute path as well as provide a more efficient representation
of the rows when given a schema. From the
<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#overview">Spark SQL, DataFrames and Datasets
Guide</a>:</p>
<blockquote>
<p>Spark SQL is a Spark module for structured data processing. Unlike the basic
Spark RDD API, the interfaces provided by Spark SQL provide Spark with more
information about the structure of both the data and the computation being
performed. Internally, Spark SQL uses this extra information to perform extra
optimizations. There are several ways to interact with Spark SQL including
SQL and the Dataset API. When computing a result the same execution engine is
used, independent of which API/language you are using to express the
computation. This unification means that developers can easily switch back
and forth between different APIs based on which provides the most natural way
to express a given transformation.</p>
</blockquote>
<p>Under the hood, these are still immutable distributed collections of data (with
the same compute graph semantics, only now Spark can apply extra
optimizations because of the (structured) format.</p>
<p>Let's do the same analysis as last time using this API. First we will define a
schema. Let's take a look at a single row of the csv:</p>
<pre><code>COHUTTA,3/10/14:1:01,10.27,1.73,881,1.56,85,1.94
</code></pre>
<p>So first a string field, a date, a timestamp, and some numeric information.
We can thus define the schema as such:</p>
<pre><code class="language-scala">val schema =
  StructType(
    Array(
      StructField(&quot;sensorname&quot;, StringType, nullable=false),
      StructField(&quot;timestamp&quot;, TimestampType, nullable=false),
      StructField(&quot;numA&quot;, DoubleType, nullable=false),
      StructField(&quot;numB&quot;, DoubleType, nullable=false),
      StructField(&quot;numC&quot;, LongType, nullable=false),
      StructField(&quot;numD&quot;, DoubleType, nullable=false),
      StructField(&quot;numE&quot;, LongType, nullable=false),
      StructField(&quot;numF&quot;, DoubleType, nullable=false)
    )
  )
</code></pre>
<p>If we import types first, and then enter this in our interactive shell we get
the following:</p>
<pre><code class="language-scala">:paste
// Entering paste mode (ctrl-D to finish)
import org.apache.spark.sql.types._
val schema =
  StructType(
    Array(
      StructField(&quot;sensorname&quot;, StringType, nullable=false),
      StructField(&quot;timestamp&quot;, TimestampType, nullable=false),
      StructField(&quot;numA&quot;, DoubleType, nullable=false),
      StructField(&quot;numB&quot;, DoubleType, nullable=false),
      StructField(&quot;numC&quot;, LongType, nullable=false),
      StructField(&quot;numD&quot;, DoubleType, nullable=false),
      StructField(&quot;numE&quot;, LongType, nullable=false),
      StructField(&quot;numF&quot;, DoubleType, nullable=false)
    )
  )


// Exiting paste mode, now interpreting.

import org.apache.spark.sql.types._
schema: org.apache.spark.sql.types.StructType =
StructType(StructField(sensorname,StringType,false),
StructField(timestamp,TimestampType,false), StructField(numA,DoubleType,false),
StructField(numB,DoubleType,false), StructField(numC,LongType,false),
StructField(numD,DoubleType,false), StructField(numE,LongType,false),
StructField(numF,DoubleType,false))
</code></pre>
<p>An overview of the different <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html">Spark SQL types</a>
can be found online. For the timestamp field we need to specify the format
according to the <a href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html">Javadate format</a>
—in our case <code>MM/dd/yy:hh:mm</code>. Tying this all together we can build a Dataframe
like so.</p>
<pre><code class="language-scala">:paste
// Entering paste mode (ctrl-D to finish)
val df = spark.read
              .schema(schema)
              .option(&quot;timestampFormat&quot;, &quot;M/d/yy:H:mm&quot;)
              .csv(&quot;./sensordata.csv&quot;)
// Exiting paste mode, now interpreting.
df: org.apache.spark.sql.DataFrame =
        [sensorname: string, timestamp: date ... 6 more fields]
</code></pre>
<pre><code class="language-scala">scala&gt; df.printSchema
root
 |-- sensorname: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- numA: double (nullable = true)
 |-- numB: double (nullable = true)
 |-- numC: long (nullable = true)
 |-- numD: double (nullable = true)
 |-- numE: long (nullable = true)
 |-- numF: double (nullable = true
</code></pre>
<pre><code class="language-scala">scala&gt; df.take(5).foreach(println)
[COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94]
[COHUTTA,2014-03-10 01:02:00.0,9.67,1.731,882,0.52,87,1.79]
[COHUTTA,2014-03-10 01:03:00.0,10.47,1.732,882,1.7,92,0.66]
[COHUTTA,2014-03-10 01:05:00.0,9.56,1.734,883,1.35,99,0.68]
[COHUTTA,2014-03-10 01:06:00.0,9.74,1.736,884,1.27,92,0.73]
</code></pre>
<p>We will now continue to perform the same filtering operation as previously
performed on the RDD.
There are three ways in which we could do this:</p>
<ol>
<li>By supplying an SQL query string to Spark SQL, operating on the <em>untyped</em>
<code>DataFrame</code>.</li>
<li>By using the Scala API for the <em>untyped</em> <code>DataFrame</code>.</li>
<li>By using the Scala API for the <em>strongly-typed</em> <code>DataSet</code>. </li>
</ol>
<h4 id="sql-query-string"><a class="header" href="#sql-query-string">SQL query string</a></h4>
<p>We can use really error prone SQL queries, like so:</p>
<pre><code class="language-scala">scala&gt; df.createOrReplaceTempView(&quot;sensor&quot;)

scala&gt; val dfFilter = spark.sql(&quot;SELECT * FROM sensor WHERE timestamp=TIMESTAMP(\&quot;2014-03-10 01:01:00\&quot;)&quot;)
dfFilter: org.apache.spark.sql.DataFrame =
            [sensorname: string, timestamp: timestamp ... 6 more fields]

scala&gt; dfFilter.collect.foreach(println)
[COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94]
[NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78]
[THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89]
...
</code></pre>
<p>As you can see, we're simply providing an SQL string to the <code>spark.sql</code> method.
The string is not checked by the Scala compiler, but only during run-time by
the Spark SQL library. Any errors will only show up during run-time.
These errors may include both typos in </p>
<ul>
<li>the SQL keywords, and</li>
<li>the field names, and</li>
<li>the timestamp.</li>
</ul>
<p>This is not recommended unless you absolutely love SQL and like debugging these
command strings. (This took me about 20 minutes to get right!)</p>
<h4 id="dataframe"><a class="header" href="#dataframe">DataFrame</a></h4>
<p>A slightly more sane and type-safe way would be to do the following:</p>
<pre><code class="language-scala">scala&gt; val dfFilter = df.filter(&quot;timestamp = TIMESTAMP(\&quot;2014-03-10 01:01:00\&quot;)&quot;)
dfFilter: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] =
                    [sensorname: string, timestamp: timestamp ... 6 more fields]

scala&gt; dfFilter.collect.foreach(println)
[COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94]
[NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78]
[THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89]
...
</code></pre>
<p>We have now replaced the SQL query of the form:</p>
<pre><code class="language-sql">SELECT fieldname WHERE predicate
</code></pre>
<p>... with the <code>filter()</code> method of a Spark DataFrame. This is already a bit
better, since the <em>Scala compiler</em> and <em>not the Spark SQL run-time library</em> can
now check the the existence of the <code>filter()</code> method for the DataFrame class.
If we made a typo, we would get a compiler error, before running the code!</p>
<p>Also, the methods supported by DataFrames look much like those of 
<a href="https://docs.scala-lang.org/overviews/parallel-collections/overview.html">Scala's parallel collections</a>,
just like with RDDs, but there are also some SQL-like database-oriented methods
such as <code>join()</code>. As such, the Scala API for DataFrames combines the best of
both worlds.</p>
<p>Still, this approach is error-prone, since it is allowed to write the filter
predicate as an SQL predicate, retaining the problem of potential errors in the
timestamp field name and the timestamp itself.</p>
<h4 id="dataset"><a class="header" href="#dataset">DataSet</a></h4>
<p>Luckily, there is also the DataSet abstraction. 
It is a sort of middle ground between DataFrames and RDDs, where you get some of
the type safety of RDDs by operating on a Scala <a href="https://docs.scala-lang.org/tour/case-classes.html">case
class</a> (also known as
product type).</p>
<p>This allows even more compile-time type checking on the product types, while
still allowing Spark to optimize the query and storage of the data by making use
of schemas.</p>
<p>We do have to write a bit more Scala to be able to use the strongly-typed
DataSet:</p>
<pre><code class="language-scala">scala&gt; import java.sql.Timestamp
import java.sql.Timestamp
</code></pre>
<pre><code class="language-scala">:paste
// Entering paste mode (ctrl-D to finish)

case class SensorData (
    sensorName: String,
    timestamp: Timestamp,
    numA: Double,
    numB: Double,
    numC: Long,
    numD: Double,
    numE: Long,
    numF: Double
)

</code></pre>
<pre><code class="language-scala">// Exiting paste mode, now interpreting.

defined class SensorData
</code></pre>
<p>Now we can convert a DataFrame (which is actually just a <code>DataSet[Row]</code>, where
<code>Row</code> allows fields to be untyped) to a typed DataSet using the <code>as</code> method.</p>
<pre><code class="language-scala">:paste
// Entering paste mode (ctrl-D to finish)

val ds = spark.read
              .schema(schema)
              .option(&quot;timestampFormat&quot;, &quot;M/d/yy:H:m&quot;)
              .csv(&quot;./sensordata.csv&quot;)
              .as[SensorData]
</code></pre>
<pre><code class="language-scala">// Exiting paste mode, now interpreting.

ds: org.apache.spark.sql.Dataset[SensorData] =
            [sensorname: string, timestamp: timestamp ... 6 more fields]
</code></pre>
<p>Now we can apply compile-time type-checked operations:</p>
<pre><code class="language-scala">scala&gt; val dsFilter = ds.filter(a =&gt; a.timestamp == Timestamp.valueOf(&quot;2014-03-10 01:01:00&quot;))
dsFilter: org.apache.spark.sql.Dataset[SensorData] =
                [sensorname: string, timestamp: timestamp ... 6 more fields]

scala&gt; dsFilter.collect.foreach(println)
SensorData(COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94)
SensorData(NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78)
SensorData(THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89)
...
</code></pre>
<p>This has two advantages:</p>
<ul>
<li>The field names can now be checked by the Scala compiler as well, by inspecting
our case class. It will detect if we made a mistake when writing <code>a.timestamp</code>.</li>
<li>The SQL-like predicate used in the DataFrame implementation is now
replaced with the constructor of the Timestamp class. This is more type-safe,
since any type mismatches will be detected by the Scala compiler. </li>
</ul>
<p>Of course, you could still supply an incorrect month number (e.g. 13). However,
the Timestamp object is already created during construction of the
lazy-evaluated DAG, not when that stage of the DAG actually starts computation.
The constructor will therefore raise any errors early on, and not at the end
stage of e.g. some computation that has been taking hours already.</p>
<p>We now have an setup where the Scala compiler will check all methods used to
build up the directed acyclic graph (DAG) of our computation exist at every
intermediate resulting DataFrame. We can't make mistakes in the SQL keywords
anymore, as well as mistakes in the field names, or data types thrown into the
filter. This provides us with more guarantees that our queries are valid (at
least at the type level).</p>
<p>There are a lot of additional advantages to DataSets that have not yet been
exposed through these examples. DataBricks has published <a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">an excellent
blog</a>
about why DataSets were introduced, next to RDDs. While DataSets don't replace
RDDs, they are nowadays most often used, because they have some more nice
properties as explained before. Read the blog to get to know the details!</p>
<p>This was a brief overview of the 2 (or 3) different Spark APIs. You can always
find more information on the programming guides for
<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDDs</a> and
<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Dataframes/Datasets</a>
and in the <a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html">Spark
documentation</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="packaging-your-application-using-sbt"><a class="header" href="#packaging-your-application-using-sbt">Packaging your application using SBT</a></h3>
<p>We showed how to run Spark in interactive mode. Now we will explain how to build
applications that can be submitted using the <code>spark-submit</code> command.</p>
<p>First, we will explain how to structure a Scala project, using the <a href="https://www.scala-sbt.org">SBT build
tool</a>. The typical project structure is</p>
<pre><code>├── build.sbt
├── project
│   └── build.properties
└── src
    └── main
        └── scala
            └── example.scala
</code></pre>
<p>This is typical for JVM languages. More directories are added under the <code>scala</code>
folder to resemble the package structure.</p>
<p>The project's name, dependencies, and versioning is defined in the <code>build.sbt</code>
file. An example <code>build.sbt</code> file is</p>
<pre><code class="language-scala">name := &quot;Example&quot;
version := &quot;0.1.0&quot;
scalaVersion := &quot;2.12.14&quot;
</code></pre>
<p>This specifies the Scala version of the project (2.12.14) and the name of the
project.</p>
<p>If you run <code>sbt</code> in this folder it will generate the project directory and
<code>build.properties</code>. <code>build.properties</code> contains the SBT version that is
used to build the project with, for backwards compatibility.</p>
<p>Open <code>example.scala</code> and add the following</p>
<pre><code class="language-scala">package example


object Example {
  def main(args: Array[String]) {
    println(&quot;Hello world!&quot;)
  }
}
</code></pre>
<p>Start the <code>sbt</code> container in the root folder (the one where <code>build.sbt</code> is
located). This puts you in interactive mode of SBT. We can compile the sources
by writing the <code>compile</code> command.</p>
<pre><code>docker run -it --rm -v &quot;`pwd`&quot;:/root sbt sbt
</code></pre>
<pre><code>copying runtime jar...
[info] welcome to sbt 1.5.5 (Oracle Corporation Java 11.0.12)
[info] loading project definition from /root/project
[info] loading settings for project root from build.sbt ...
[info] set current project to Example (in build file:/root/)
[info] sbt server started at local:///root/.sbt/1.0/server/27dc1aa3fdf4049b492d/sock
[info] started sbt server
sbt:Example&gt;
</code></pre>
<p>We can now type <code>compile</code>.</p>
<pre><code>sbt:Example&gt; compile
[info] compiling 1 Scala source to /root/target/scala-2.12/classes ...
...
[info] Non-compiled module 'compiler-bridge_2.12' for Scala 2.12.14. Compiling...
[info]   Compilation completed in 10.128s.
[success] Total time: 14 s, completed Aug 26, 2021, 3:03:34 PM
</code></pre>
<p>We can try to run the application by typing <code>run</code>.</p>
<pre><code>sbt:Example&gt; run
[info] running example.Example
Hello world!
[success] Total time: 1 s, completed Aug 26, 2021, 3:05:29 PM
</code></pre>
<p>Now let's add a function to <code>example.scala</code>.</p>
<pre><code class="language-scala">object Example {
  def addOne(tuple: (Char, Int)) : (Char, Int) = tuple match {
    case (chr, int) =&gt; (chr, int+1)
  }
  def main(args: Array[String]) {
    println(&quot;Hello world!&quot;)
    println(addOne('a', 1))
  }
}
</code></pre>
<p>In your SBT session we can prepend any command with a tilde (<code>~</code>) to make them
run automatically on source changes.</p>
<pre><code>sbt:Example&gt; ~run
[info] Compiling 1 Scala source to /root/target/scala-2.12/classes ...
[info] running example.Example
Hello world!
(a,2)
[success] Total time: 0 s, completed Sep 7, 2020 10:40:56 AM
[info] 1. Monitoring source files for root/run...
[info]    Press &lt;enter&gt; to interrupt or '?' for more options.

</code></pre>
<p>We can also open an interactive session using SBT.</p>
<pre><code>sbt:Example&gt; console
[info] Starting scala interpreter...
Welcome to Scala 2.12.14 (OpenJDK 64-Bit Server VM, Java 11.0.12).
Type in expressions for evaluation. Or try :help.

scala&gt; example.Example.addOne('a', 1)
res1: (Char, Int) = (a,2)

scala&gt; println(&quot;Interactive environment&quot;)
Interactive environment
</code></pre>
<p>To build Spark applications with SBT we need to include dependencies (Spark
most notably) to build the project. Modify your <code>build.sbt</code> file like so</p>
<pre><code class="language-scala">name := &quot;Example&quot;
version := &quot;0.1.0&quot;
scalaVersion := &quot;2.12.14&quot;

val sparkVersion = &quot;3.1.2&quot;

libraryDependencies ++= Seq(
  &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % sparkVersion,
  &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % sparkVersion
)
</code></pre>
<p>If you still have the SBT shell opened, you must use <code>reload</code> to make sure your
<code>build.sbt</code> is updated.</p>
<p>We could now use Spark in the script (after running <code>compile</code>).</p>
<p>Let's implement a Spark application.
Modify <code>example.scala</code> as follows, but don't run the code yet!</p>
<pre><code class="language-scala">package example

import org.apache.spark.sql.types._
import org.apache.spark.sql._
import java.sql.Timestamp


object ExampleSpark {
  case class SensorData (
    sensorName: String,
    timestamp: Timestamp,
    numA: Double,
    numB: Double,
    numC: Long,
    numD: Double,
    numE: Long,
    numF: Double
  )
  def main(args: Array[String]) {
    val schema =
      StructType(
        Array(
          StructField(&quot;sensorname&quot;, StringType, nullable=false),
          StructField(&quot;timestamp&quot;, TimestampType, nullable=false),
          StructField(&quot;numA&quot;, DoubleType, nullable=false),
          StructField(&quot;numB&quot;, DoubleType, nullable=false),
          StructField(&quot;numC&quot;, LongType, nullable=false),
          StructField(&quot;numD&quot;, DoubleType, nullable=false),
          StructField(&quot;numE&quot;, LongType, nullable=false),
          StructField(&quot;numF&quot;, DoubleType, nullable=false)
        )
      )

    val spark = SparkSession
      .builder
      .appName(&quot;Example&quot;)
      .getOrCreate()
    val sc = spark.sparkContext // If you need SparkContext object

    import spark.implicits._

    val ds = spark.read
                  .schema(schema)
                  .option(&quot;timestampFormat&quot;, &quot;M/d/yy:H:m&quot;)
                  .csv(&quot;./sensordata.csv&quot;)
                  .as[SensorData]

    val dsFilter = ds.filter(a =&gt; a.timestamp ==
        Timestamp.valueOf(&quot;2014-03-10 01:01:00&quot;))

    dsFilter.collect.foreach(println)

    spark.stop
  }
}
</code></pre>
<p>We will not run this code, but submit it to a local Spark &quot;cluster&quot; (on your
machine). To do so, we require a JAR. You can build a JAR using the <code>package</code>
command (or <code>assembly</code> to include all dependencies) in SBT. This JAR will be located in the
<code>target/scala-version/project_name_version.jar</code>.</p>
<p>You can run the JAR via a <code>spark-submit</code> container (which will run on local
mode). By mounting the <code>spark-events</code> directory the event log of the
application run is stored to be inspected later using the Spark history server.</p>
<pre><code>docker run -it --rm -v &quot;`pwd`&quot;:/io -v &quot;`pwd`&quot;/spark-events:/spark-events spark-submit target/scala-2.12/example_2.12-0.1.0.jar
</code></pre>
<p>The output should look as follows:</p>
<pre><code>2020-09-07 11:07:28,890 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-07 11:07:29,068 INFO spark.SparkContext: Running Spark version 3.1.2
2020-09-07 11:07:29,087 INFO spark.SparkContext: Submitted application: Example

...

SensorData(COHUTTA,2014-03-10 01:01:00.0,10.27,1.73,881,1.56,85,1.94)
SensorData(NANTAHALLA,2014-03-10 01:01:00.0,10.47,1.712,778,1.96,76,0.78)
SensorData(THERMALITO,2014-03-10 01:01:00.0,10.24,1.75,777,1.25,80,0.89)
SensorData(BUTTE,2014-03-10 01:01:00.0,10.12,1.379,777,1.58,83,0.67)
SensorData(CARGO,2014-03-10 01:01:00.0,9.93,1.903,778,0.55,76,1.44)
SensorData(LAGNAPPE,2014-03-10 01:01:00.0,9.59,1.602,777,0.09,88,1.78)
SensorData(CHER,2014-03-10 01:01:00.0,10.17,1.653,777,1.89,96,1.57)
SensorData(ANDOUILLE,2014-03-10 01:01:00.0,10.26,1.048,777,1.88,94,1.66)
SensorData(MOJO,2014-03-10 01:01:00.0,10.47,1.828,967,0.36,77,1.75)
SensorData(BBKING,2014-03-10 01:01:00.0,10.03,0.839,967,1.17,80,1.28)

...

2020-09-07 11:07:33,694 INFO util.ShutdownHookManager: Shutdown hook called
2020-09-07 11:07:33,694 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-757daa7c-c317-428e-934f-aaa9e74bf808
2020-09-07 11:07:33,696 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a38554ba-18fc-46aa-aa1e-0972e24a4cb0
</code></pre>
<p>By default, Spark's logging is quite verbose. You can change the <a href="https://stackoverflow.com/questions/27781187/how-to-stop-info-messages-displaying-on-spark-console">log levels
to warn</a>
to reduce the output.</p>
<p>For development purposes you can also try running the application from SBT
using the <code>run</code> command. Make sure to set the Spark master to <code>local</code> in your
code. You might run into some trouble with threads here, which can be solved
by running the application in a forked process, which can be enabled by
setting <code>fork in run := true</code> in <code>build.sbt</code>. You will also have to set to
change the log levels programmatically, if desired.</p>
<pre><code class="language-scala">import org.apache.log4j.{Level, Logger}
...


def main(args: Array[String]) {
    ...
    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
    ...
}
</code></pre>
<p>You can also use this logger to log your application which might be helpful for
debugging on the AWS cluster later on.</p>
<p>You can inspect the event log from the application run using the Spark history
server. Start a <code>spark-history-server</code> container from the project root folder
and mount the <code>spark-events</code> folder in the container.</p>
<pre><code>docker run -it --rm -v &quot;`pwd`&quot;/spark-events/:/spark-events -p 18080:18080 spark-history-server
</code></pre>
<p>The output will look as follows:</p>
<pre><code>starting org.apache.spark.deploy.history.HistoryServer, logging to /spark/logs/spark--org.apache.spark.deploy.history.HistoryServer-1-5b5de5805769.out

...

2020-09-07 11:10:23,020 INFO history.FsHistoryProvider: Parsing file:/spark-events/local-1599477015931 for listing data... 2020-09-07
11:10:23,034 INFO history.FsHistoryProvider: Finished parsing file:/spark-events/local-1599477015931
</code></pre>
<p>Navigate to <a href="localhost:18080">http://localhost:18080</a> to view detailed
information about your jobs.
After analysis you can shutdown the Spark history server using ctrl+C.</p>
<pre><code>^C
2020-09-07 11:13:21,619 ERROR history.HistoryServer: RECEIVED SIGNAL INT
2020-09-07 11:13:21,630 INFO server.AbstractConnector: Stopped Spark@70219bf{HTTP/1.1,[http/1.1]}{0.0.0.0:18080}
2020-09-07 11:13:21,633 INFO util.ShutdownHookManager: Shutdown hook called
</code></pre>
<p>Be sure to explore the history server thoroughly! You can use it to gain an
understanding of how Spark executes your application, as well as to debug and
time your code, which is important for both lab 1 and 2.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="amazon-web-services-1"><a class="header" href="#amazon-web-services-1">Amazon Web Services</a></h2>
<p>AWS consists of a variety of different services, the ones relevant for this lab
are listed below:</p>
<ul>
<li><a href="https://aws.amazon.com/ec2/">Elastic Compute Cloud</a> allows you to provision 
a variety of different machines that can be used to run a computation. An 
overview of the different machines and their use cases can be found on the EC2 
website.</li>
<li><a href="https://aws.amazon.com/emr/">Elastic MapReduce</a> is a layer on top of EC2, 
that allows you to quickly deploy MapReduce-like applications, for instance 
Apache Spark.</li>
<li><a href="https://aws.amazon.com/s3/">Simple Storage Server</a> is an object based storage
system that is easy to interact with from different AWS services.</li>
</ul>
<p>AWS EC2 offers spot instances, a marketplace for unused machines that you can
bid on. These spot instances are often an order of magnitude cheaper than
on-demand instances. The current price list can be found in the 
<a href="https://aws.amazon.com/ec2/spot/pricing/">EC2 website</a>. We recommend using
spot instances for the entirety of this lab.
However, be aware that rarely AWS will reclaim a spot instance because someone
is willing to pay more for it as an on-demand instance.
When this happens, your cluster/job may fail.</p>
<h3 id="uploading-your-application"><a class="header" href="#uploading-your-application">Uploading your application</a></h3>
<p>We will be using the AWS infrastructure to run the application. Log in to the 
AWS console, and open the S3 interface. Create a bucket where we can store the
application JAR, and all the other files needed by your application.</p>
<p>There are (at least) two ways to transfer files to S3:</p>
<ol>
<li>The web interface, and</li>
<li>The command line interface.</li>
</ol>
<p>The web interface is straightforward to use. To use the command line interface,
first install the <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">AWS CLI</a>.
Some example operations are listed below.</p>
<p>To copy a file</p>
<pre><code class="language-bash">aws s3 cp path/to/file s3://destination-bucket/path/to/file
</code></pre>
<p>To copy a directory recursively</p>
<pre><code class="language-bash">aws s3 cp --recursive s3://origin-bucket/path/to/file
</code></pre>
<p>To move a file</p>
<pre><code class="language-bash">aws s3 mv path/to/file s3://destination-bucket/path/to/file
</code></pre>
<p>The aws-cli contains much more functionality, which can be found on the
<a href="https://aws.amazon.com/cli/">AWS-CLI docs</a>.</p>
<h3 id="creating-the-cluster"><a class="header" href="#creating-the-cluster">Creating the cluster</a></h3>
<p>Once you have uploaded all the necessary files (again your application JAR, and
all the files required by the application), we are ready to provision a
cluster. Go to the EMR service, and select <em>Create Cluster</em>. Next select <em>Go to
advanced options</em>, select the latest release, and check the frameworks you want
to use. In this case this means Spark, Hadoop and Ganglia. Spark and Hadoop you
already know, we will introduce Ganglia later in this chapter.</p>
<p>EMR works with steps, which can be thought of as a job, or the execution of a
single application. You can choose to add steps in the creation of the cluster,
but this can also be done at a later time. Press <em>next</em>.</p>
<p>In the <em>Hardware Configuration</em> screen, we can configure the arrangement and
selection of the machines. We suggest starting out with <code>m4.large</code> machines on
spot pricing. You should be fine running a small example workload with a single
master node and two core nodes. Be sure to select <em>spot pricing</em> and
place an appropriate bid. Remember that you can always check the current prices
in the information popup or on the <a href="https://aws.amazon.com/ec2/spot/pricing/">ec2 website</a>.
After selecting the machines, press <em>next</em>.</p>
<p>Please note:</p>
<ul>
<li>
<p>You always need a master node, which is tasked with distributing
resources and managing tasks for the core nodes. We recommend using
the cheap <code>m4.large</code> instance. If you start to notice unexplained
bottlenecks for tasks with many machines and a lot of data, you might want
to try a larger master node. Ganglia should provide you with some insights
regarding this matter.</p>
</li>
<li>
<p>By default, there are some limitations on the number of spot instances
your account is allowed to provision. If you don't have access to enough
spot instances, the procedure to request additional can be found in the
<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-limits.html">AWS documentation</a>.</p>
</li>
</ul>
<p>In the <em>General Options</em> you can select a cluster name. Name it after your 
group, e.g. <code>group-01</code>. You can tune where the system logs and a number of other 
features (more information in the popups). After finishing this step, press 
<em>next</em>.</p>
<p>You should now arrive in the <em>Security Options</em> screen. If you have not created
an <em>EC2 keypair</em>, it is recommended that you do so now. This will allow you to
access the Yarn, Spark, and Ganglia 
<a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html">web interfaces</a>
in your browser. This makes debugging and monitoring the execution of your Spark
Job much more manageable. To create an <em>EC2 keypair</em>, follow 
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html">these instructions</a>
.</p>
<p>After this has all been completed you are ready to spin up your first cluster
by pressing <em>Create cluster</em>. Once the cluster has been created, AWS will start
provisioning machines. This should take about 10 minutes. In the meantime you
can add a step. Go to the <em>Steps</em> foldout, and select <em>Spark application</em> for
<em>Step Type</em>. Clicking on <em>Configure</em> will open a dialogue in which you can
select the application JAR location in your S3 bucket, as well as any number
of arguments to the application, spark-submit, as well as your action on
failure.</p>
<p>The setup will take some time to finish, so in the meantime you should
configure a proxy for the web interfaces. More detailed information can be
found on the <a href="http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html">AWS website</a>. 
You can check the logs in your S3 bucket, or the web interfaces to track the 
progress of your application and whether any errors have occurred.</p>
<p>By forwarding the web interfaces you will also have access to Apache Ganglia.
Ganglia is a tool that allows you to monitor your cluster for incoming and
outgoing network, CPU load, memory pressure, and other useful metrics. They can
help to characterize the workload at hand, and help optimizing computation
times. An example of its interface is shown in the figure below.</p>
<p><img src="getting-started/../assets/images/ganglia.png" alt="Ganglia screenshot" /></p>
<p>It's not uncommon to run into problems when you first deploy your application
to AWS, here are some general clues:</p>
<ul>
<li>
<p>You can access S3 files directly using Spark, so via
<code>SparkContext.textFile</code> and <code>SparkSession.read.csv</code>, but not using the OS,
so using an ordinary <code>File</code> java class will not work. If you want to load a
file to the environment, you will have to figure out a workaround.</p>
</li>
<li>
<p>You can monitor the (log) output of your master and worker nodes in Yarn,
which you can access in the web interfaces. It might help you to insert
some helpful logging messages in your Application.</p>
</li>
<li>
<p>Scale your application by increasing the workload by an order of magnitude
at a time, some bugs only become apparent when you have a sufficient load
on your cluster and a sufficient cluster size. In terms of cost, it's also
much cheaper if you do your debugging incrementally on smaller clusters.</p>
</li>
<li>
<p>Ensure that your cluster is running in actual cluster mode (can be visually
confirmed by checking the load on the non-master nodes in Ganglia).</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="apache-kafka-1"><a class="header" href="#apache-kafka-1">Apache Kafka</a></h2>
<p>Apache Kafka is a distributed streaming platform. The core abstraction is that
of a message queue, to which you can both publish and subscribe to streams of
records. Each queue is named by means of a topic. Apache Kafka is:</p>
<ul>
<li>Resilient by means of replication;</li>
<li>Scalable on a cluster;</li>
<li>High-throughput and low-latency; and</li>
<li>A persistent store.</li>
</ul>
<p>Kafka consists of 4 APIs, from the Kafka docs:</p>
<h3 id="the-producer-api"><a class="header" href="#the-producer-api">The Producer API</a></h3>
<p>Allows an application to publish a stream of records to one or more Kafka
topics.</p>
<h3 id="the-consumer-api"><a class="header" href="#the-consumer-api">The Consumer API</a></h3>
<p>Allows an application to subscribe to one or more topics and process the
stream of records produced to them.</p>
<h3 id="the-streams-api"><a class="header" href="#the-streams-api">The Streams API</a></h3>
<p>Allows an application to act as a stream processor, consuming an input
stream from one or more topics and producing an output stream to one or
more output topics, effectively transforming the input streams to output
streams.</p>
<h3 id="the-connector-api"><a class="header" href="#the-connector-api">The Connector API</a></h3>
<p>Allows building and running reusable producers or consumers that connect
Kafka topics to existing applications or data systems. For example, a
connector to a relational database might capture every change to a table.</p>
<p>Before you start with the lab, please read the <a href="https://kafka.apache.org/intro">Introduction to Kafka on the Kafka
website</a>, to become familiar with the Apache
Kafka abstraction and internals. A good introduction to the <a href="https://docs.confluent.io/current/streams/quickstart.html">Kafka stream API can be found
here</a>. We recommend
you go through the code and examples.</p>
<p>We will again be using Scala for this assignment. Although Kafka's API is
completely written in Java, the streams API has been wrapped in a Scala API for
convenience. You can find the Scala KStreams documentation
<a href="https://developer.lightbend.com/docs/api/kafka-streams-scala/0.2.1/com/lightbend/kafka/scala/streams/KStreamS.html">here</a>,
for API docs on the different parts of Kafka, like <code>StateStores</code>, please refer
to <a href="https://kafka.apache.org/23/javadoc/overview-summary.html">this link</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="openstreetmap-1"><a class="header" href="#openstreetmap-1">OpenStreetMap</a></h2>
<p><a href="https://www.openstreetmap.org">OpenStreetMap</a> is a project where you can download free geographic data from
the whole world. Such data will be used as the only data source for our queries
in this lab. The project has an excellent <a href="https://wiki.openstreetmap.org/wiki/Main_Page">Wiki</a> where a lot of information
about the structure of the dataset may be found.</p>
<p>OpenStreetMap data knows three map elements:</p>
<ul>
<li><a href="https://wiki.openstreetmap.org/wiki/Node">Nodes</a> : a single point on the map, e.g. to mark the location of a <a href="https://www.openstreetmap.org/node/4829046021">brewery</a>.</li>
<li><a href="https://wiki.openstreetmap.org/wiki/Way">Ways</a> : an ordered lists of nodes, e.g. to represent (a part of) a <a href="https://www.openstreetmap.org/way/7624546">street</a>.</li>
<li><a href="https://wiki.openstreetmap.org/wiki/Relation">Relations</a> : a group of other elements, e.g. to represent the <a href="https://www.openstreetmap.org/relation/47798">boundary</a> and
center of a city.</li>
</ul>
<p>All map elements may have associated <a href="https://wiki.openstreetmap.org/wiki/Tags">tags</a> that expose more information about
what they represent. For example, we may look at <a href="https://www.openstreetmap.org/node/3376839743">some node</a>, and find that it
represents the Delft central station, with the following tags:</p>
<table><thead><tr><th>Key</th><th>Value</th></tr></thead><tbody>
<tr><td>name</td><td>Delft</td></tr>
<tr><td>public_transport</td><td>station</td></tr>
<tr><td>railway</td><td>station</td></tr>
<tr><td>railway:ref</td><td>Dt</td></tr>
<tr><td>train</td><td>yes</td></tr>
<tr><td>wheelchair</td><td>yes</td></tr>
<tr><td>wikidata</td><td>Q800653</td></tr>
<tr><td>wikipedia</td><td>nl:Station Delft</td></tr>
</tbody></table>
<p>Many tags have an explanation on the Wiki, for example the <a href="https://wiki.openstreetmap.org/wiki/Key:wheelchair">wheelchair</a> tag,
where its value describes an indication of the level of accessibility for people
in wheelchairs to, in this case, the station.</p>
<p>Feel free to browse around the wiki to discover other tags. We can spoil that
<a href="https://wiki.openstreetmap.org/wiki/Brewery">this</a> page contains some useful tags that you may want to use for your lab
assignments.</p>
<h3 id="preparing-the-dataset-for-lab-1"><a class="header" href="#preparing-the-dataset-for-lab-1">Preparing the dataset for Lab 1</a></h3>
<p>We will now explain how to prepare our dataset for lab 1. Because the query that
we are interested in is too large to process during our initial short
development iterations of implementing and debugging, its useful to start off
with a small subset of the data (in lab 1), until your implementation is stable
enough to be moved to a large cluster (in lab 2), to process the whole thing!</p>
<p>Furthermore, to be able to leverage SparkSQL, we must convert the OpenStreetMap
data to a tabular format. We will use the ORC format for this. Follow the steps
below to end up with an ORC file for lab 1.</p>
<p>Luckily, some people have already chopped up the whole world into manageable
pieces. In our case, we will start off with just the Netherlands,
where Delft is located. Also, some people have written <a href="https://github.com/mojodna/osm2orc">a conversion tool</a> for
us already, that helps us convert the <code>.osm.pbf</code> file into an <code>.orc</code> file.</p>
<ul>
<li>
<p>Download the <a href="https://download.geofabrik.de/europe.html">Netherlands</a>. You need to get the <code>.osm.pbf</code> file.</p>
</li>
<li>
<p>Download and extract v.0.5.5 of <a href="https://github.com/mojodna/osm2orc/releases/download/v0.5.5/osm2orc-0.5.5.tar.gz">osm2orc</a> (click the link to download
immediately).</p>
</li>
<li>
<p>Run osm2orc to obtain the ORC file of the Netherlands. osm2orc can be found in
the <code>bin/</code> folder of the tool. Example usage in Linux:</p>
</li>
</ul>
<pre><code class="language-console">./osm2orc /path/to/netherlands-latest.osm.pbf /destination/for/netherlands.orc
</code></pre>
<p>You can also use the Dockerfile provided in the lab 1 repository to build an image
that includes this tool:</p>
<ul>
<li>First build the <code>osm2orc</code> image (from the root of the lab 1 repository):</li>
</ul>
<pre><code class="language-bash">docker build --target osm2orc -t osm2orc .
</code></pre>
<ul>
<li>Run the conversion tool in a container. Make sure the source <code>.osm.pbf</code> file is
downloaded in your current working directory.</li>
</ul>
<pre><code class="language-bash">docker run -it --rm -v &quot;`pwd`&quot;:/io osm2orc /io/netherlands-latest.osm.pbf /io/netherlands.orc
</code></pre>
<p>You will now have the <code>netherlands.orc</code> file somewhere on your machine. We will
use this file as our input. Make sure you understand <a href="http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html">how to load the file into
Spark</a>.</p>
<h3 id="openstreetmap-schema"><a class="header" href="#openstreetmap-schema">OpenStreetMap schema</a></h3>
<p>Once you have loaded the data set, it's possible to print out the schema of the
data, for example, on the Spark shell. Note that this assumes you have loaded
the data set as a Spark DataFrame called <code>df</code>.</p>
<pre><code>scala&gt; df.printSchema()
root
 |-- id: long (nullable = true)
 |-- type: string (nullable = true)
 |-- tags: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)
 |-- lat: decimal(9,7) (nullable = true)
 |-- lon: decimal(10,7) (nullable = true)
 |-- nds: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- ref: long (nullable = true)
 |-- members: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- type: string (nullable = true)
 |    |    |-- ref: long (nullable = true)
 |    |    |-- role: string (nullable = true)
 |-- changeset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- uid: long (nullable = true)
 |-- user: string (nullable = true)
 |-- version: long (nullable = true)
 |-- visible: boolean (nullable = true)
</code></pre>
<p>And we may look at a tiny part of the data, say the first three rows:</p>
<pre><code>scala&gt; df.show(3)
+-------+----+----+----------+---------+---+-------+---------+-------------------+---+----+-------+-------+
|     id|type|tags|       lat|      lon|nds|members|changeset|          timestamp|uid|user|version|visible|
+-------+----+----+----------+---------+---+-------+---------+-------------------+---+----+-------+-------+
|2383199|node|  []|52.0972000|4.2997000| []|     []|        0|2011-10-31 12:45:53|  0|    |      3|   true|
|2383215|node|  []|52.0990543|4.3002070| []|     []|        0|2020-01-08 20:50:29|  0|    |      6|   true|
|2716646|node|  []|52.0942524|4.3354580| []|     []|        0|2011-12-26 23:12:28|  0|    |      3|   true|
+-------+----+----+----------+---------+---+-------+---------+-------------------+---+----+-------+-------+
</code></pre>
<p>Here we can observe that the first three rows are elements of the <code>&quot;node&quot;</code> type.
Note that there are two other types of elements, as explained previously. We
also have the <code>&quot;way&quot;</code> and <code>&quot;relation&quot;</code>. There are all flattened into this single
table by the conversion tool.</p>
<p>How big is this dataframe?</p>
<pre><code>scala&gt; df.count()
res1: Long = 130506997
</code></pre>
<p>It has over 130 million rows already, and this is just the Netherlands!</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="a-hrefhttpswwweorcjaxajpalosenaw3d30indexhtmalos-global-digital-surface-modela"><a class="header" href="#a-hrefhttpswwweorcjaxajpalosenaw3d30indexhtmalos-global-digital-surface-modela"><a href="https://www.eorc.jaxa.jp/ALOS/en/aw3d30/index.htm">ALOS Global Digital Surface Model</a></a></h2>
<p>This dataset contains elevation information about the whole surface of the
earth. The data set consist of a bunch of files that are formatted using the
GeoTIFF format. The data is split up over tiles, where each tile covers a unit
area of 1 degree latitude and longitude of the earths surface.</p>
<p>Because Spark does not know how to work with such files out of the box, we
provide you with the <a href="https://github.com/mbrobbel/aw3d30-parquet">aw3d30-parquet</a> tool that helps us pre-process this data.
The tool converts the GeoTIFF data to an Apache Parquet file. Apache Parquet
files hold tabular datastructures. This makes it easier to work with when using
various Spark APIs.</p>
<pre><code>message schema {
  REQUIRED DOUBLE lat;
  REQUIRED DOUBLE lon;
  REQUIRED INT32 elevation;
}
</code></pre>
<p>Coordinates are in the <a href="https://en.wikipedia.org/wiki/World_Geodetic_System#WGS84">WGS84</a> reference schema and elevation is in meters.</p>
<p>To produce a Parquet file from the data, first build a Docker image or build and install the tool from <a href="https://github.com/mbrobbel/aw3d30-parquet">source</a>.</p>
<pre><code>docker build -t aw3d30 https://github.com/mbrobbel/aw3d30-parquet.git#main
</code></pre>
<p>Once you've build the image, you can run it as follows:</p>
<pre><code>docker run -it --rm -v &quot;`pwd`&quot;:/io aw3d30 -t /io/tif -p /io/parquet &lt;set&gt;
</code></pre>
<p>Or when built from source:</p>
<pre><code>aw3d30-parquet &lt;set&gt;
</code></pre>
<p>Where <code>&lt;set&gt;</code> is one of <code>netherlands</code>, <code>europe</code> or <code>world</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-1"><a class="header" href="#lab-1">Lab 1</a></h1>
<p><code>Workload: approx. 65 hours per student</code></p>
<p>In this lab, we will design and develop an application to process
the <a href="https://www.openstreetmap.org">OpenStreetMap</a> and <a href="https://www.eorc.jaxa.jp/ALOS/en/aw3d30/index.htm">ALOS Global Digital Surface Model</a> data sets for a
tiny subset of the planet (the Netherlands).</p>
<p>This lab will not yet require significant resources in terms of compute, memory
or storage. It should be able to perform this lab on a laptop. The first lab is
mainly meant to familiarize yourself with the toolchains and the data sets, and
perhaps Scala. We will:</p>
<ol>
<li>get familiar with the Spark APIs,</li>
<li>analyze the application's scaling behavior, and</li>
<li>draw some conclusions on how to run it efficiently at scale for lab 2</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="before-you-start"><a class="header" href="#before-you-start">Before you start</a></h2>
<p>We recommend you read all the relevant sections on Scala and Spark in the
guide. Make sure you have Docker (or the required tools) up-and-running and that
you have built the required images for Spark and SBT, as per the instructions.
You can verify your set-up by going through the steps of the Spark tutorial.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="assignment"><a class="header" href="#assignment">Assignment</a></h2>
<p>The requirements of the application that you must implement are described below
in three tiers (&quot;adequate&quot;, &quot;good&quot; and &quot;excellent&quot;). These tiers correspond to
one of the grading criteria (explained on the next page). To pass the lab, you
must at least achieve the &quot;adequate&quot; tier. &quot;Good&quot; and &quot;excellent&quot; tier
applications contain <strong>additional</strong> requirements (and may supersede previous
requirements where applicable) that the program needs to adhere to.</p>
<p>The goal of this lab is to make an application which efficiently determines how many people would need to relocate at different amount of sea level rise. </p>
<h3 id="adequate-application-requirements"><a class="header" href="#adequate-application-requirements">&quot;Adequate&quot; application requirements</a></h3>
<ol>
<li>The application takes an integer amount of sea level rise in meters from the command line as the first positional argument.</li>
<li>The application uses the ALOS Global Digital Surface Model dataset to determine which places are exactly at or below the new sea level, you can disregard any man-made or natural water barriers. So if the sea level rises 0 meters, everybody living in a place at elevation of 0 meters or lower has to relocate.</li>
<li>The application uses the OpenStreetMap dataset to determine populations. Only cities, towns, villages and hamlets are used to determine if the population needs to relocate. Boroughs, suburbs, quarters etc. are parts of cities and should not be considered separately. The whole population of a city/town/village/hamlet can be assumed to live at the exact coordinates of the place in the OSM dataset.</li>
<li>When determining which place lies at which elevation, the application uses the <a href="https://github.com/uber/h3">H3: Uber’s Hexagonal Hierarchical Spatial Index</a>
to reduce the computational complexity of joins between the OSM and ALOS data sets. In the report (README.md), it is explicitly explained why using such a spatial index reduces the computational complexity for these types of joins.</li>
<li>The application uses the average elevation of an H3 tile to determine if a population needs to relocate.</li>
<li>The application uses an H3 resolution of 9.</li>
<li>The application outputs the sum of all evacuees on the command line.</li>
<li>The application outputs an <code>.orc</code> file with the following schema, where <code>place</code> is the name of a city/town/village/hamlet, and <code>num_evacuees</code> is the number of people in this place:</li>
</ol>
<pre><code class="language-scala">import org.apache.spark.sql.types._

val schema = StructType(
               Array(
                 StructField(&quot;place&quot;, StringType),
                 StructField(&quot;num_evacuees&quot;, LongType)
               )
             )
</code></pre>
<h3 id="good-application-requirements"><a class="header" href="#good-application-requirements">&quot;Good&quot; application requirements</a></h3>
<ol start="9">
<li>Produce a relocation plan which is a mapping from source place to the
closest safe (one that is not underwater) city (not town or village or
hamlet). For the distance calculation, consider the earth to be flat (like
🥞), i.e. you don't have to take the curvature of the earth into
consideration. In this case, the application outputs a modified <code>.orc</code> file
with the following schema, where <code>place</code> is the name of a
city/town/village/hamlet, <code>num_evacuees</code> is the number of people in this
place, and <code>destination</code> is the name of the city to relocate to.</li>
</ol>
<pre><code class="language-scala">import org.apache.spark.sql.types._

val schema = StructType(
               Array(
                 StructField(&quot;place&quot;, StringType),
                 StructField(&quot;num_evacuees&quot;, LongType)
                 StructField(&quot;destination&quot;, StringType)
               )
             )
</code></pre>
<h3 id="excellent-application-requirements"><a class="header" href="#excellent-application-requirements">&quot;Excellent&quot; application requirements</a></h3>
<ol start="10">
<li>The application also calculates the distance of each place to the closest
harbour.</li>
<li>If a harbour is closer than a safe city, 25% of the population of the place
must relocate to the harbour to get a boat and live on the ocean. The other 
75% will still relocate to the nearest safe city. The application outputs a 
modified <code>.orc</code> file with the following schema, where <code>place</code> is the name of 
a city/town/village/hamlet, <code>num_evacuees</code> is the number of people 
evacuating to the <code>destination</code>, which is the name of the city to relocate
to or <code>Waterworld</code> in the case of boat relocation:</li>
</ol>
<pre><code class="language-scala">import org.apache.spark.sql.types._

val schema = StructType(
               Array(
                 StructField(&quot;place&quot;, StringType),
                 StructField(&quot;num_evacuees&quot;, LongType)
                 StructField(&quot;destination&quot;, StringType)
               )
             )
</code></pre>
<ol start="13">
<li>The application produces a list of cities that are to be relocated to,
including <code>Waterworld</code> as if it's a city. The list also provides the old
population and the new population The application outputs this list as a
secondary <code>.orc</code> file with the following schema, where <code>destination</code> is the
name of the city (or <code>Waterworld</code>) that will receive evacuees,
<code>old_population</code> is the population before relocation, and <code>new_population</code>
is the population after relocation.</li>
</ol>
<pre><code class="language-scala">import org.apache.spark.sql.types._

val schema = StructType(
               Array(
                 StructField(&quot;destination&quot;, StringType),
                 StructField(&quot;old_population&quot;, LongType)
                 StructField(&quot;new_population&quot;, LongType)
               )
             )
</code></pre>
<h3 id="input"><a class="header" href="#input">Input</a></h3>
<p>For this lab, we will limit ourselves to answer the above questions for
the <a href="http://download.geofabrik.de/europe/netherlands-latest.osm.pbf">Netherlands</a> subset of OpenStreetMap (provided by <a href="https://geofabrik.de/">Geofabrik</a>) and ALOS.
The <code>.osm.pbf</code> files can be converted to ORC files using the tool mentioned in
the introduction. The ALOS tool mentioned in the introduction can be used to
obtain Parquet files with elevation data.</p>
<p><strong>For the sea level rise, you should report your output for <code>0</code>, <code>10</code>, and <code>100</code>
meters,</strong> but feel free to experiment with other levels.</p>
<h3 id="hints"><a class="header" href="#hints">Hints</a></h3>
<p>Try to keep the following in mind when building your application:</p>
<ul>
<li>Functionality: providing a correct answer to this query is not trivial,
attempt to make your application as robust as possible.</li>
<li>Scalable: your application should keep functioning correctly when the size of
the input data set grows larger.</li>
<li>Use H3 version 3.7.0, since higher versions may cause problems in the
container setup.</li>
<li>If you use all data while developing and debugging, testing a single code
change may take very long, e.g. more than 10 minutes. Therefore, while
developing, try to make your iterations as short as possible by limiting the
input datasets. For example, you could first try to answer the question only
for cities of a population higher than some large amount (so you end up
answering the query for a few cities, rather than for thousands of places).
Or, you could load only one or two ALOS tiles of which you know includes a few
places, so you end up with less elevation data. Once you have a functional
implementation, move towards calculating on the whole dataset to report the
output and the run times.</li>
<li>To get an idea of where a lot of computation takes place, make sure to check
the Spark history server. This helps you think about how to improve expensive
steps such as shuffles, by reordering or caching computations, or
pre-processing data sets, or by thinking about when to include pieces of data
in intermediate dataframes/sets.</li>
<li>When writing ORC files, multiple files will be created. This is fine, because
Spark saves each partition separately and for a typical deployment it does so 
in a distributed manner. When a requirement says to write &quot;an ORC file&quot;, it is
OK if you end up with multiple files.</li>
<li>For 0 meter sealevel rise, a whole lot of people should move. This is obviously not very realistic. If you want to see if you can get a more realistic answer, you can do so and put it in the report, might be worth some bonus points. But make sure you also show that you did the exercise as written.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="deliverables"><a class="header" href="#deliverables">Deliverables</a></h2>
<p>The deliverables for this lab are:</p>
<ol>
<li>Spark application</li>
<li>Report</li>
</ol>
<p>Deliverables should be contained in your project repository.</p>
<h3 id="spark-application"><a class="header" href="#spark-application">Spark Application</a></h3>
<p>All Scala source files required to build your Spark application should be
committed to your repository.</p>
<h3 id="report"><a class="header" href="#report">Report</a></h3>
<p>The results and analysis in the <code>README.md</code> markdown file in the root of your
repository.</p>
<p>The report should include:</p>
<ul>
<li>The output of the <code>adequate</code> application for a sea level rise of <code>0</code>, <code>10</code>, and <code>100</code> meters, as a table with the <em>total</em> number of evacuees for these three sea levels.</li>
<li>What the highest level of the application is that you implemented.</li>
<li>The final runtime of your implementation.</li>
</ul>
<p>As well as the requirements as stated in the <code>Rubric</code> section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rubric-for-lab-1"><a class="header" href="#rubric-for-lab-1">Rubric for Lab 1</a></h1>
<h2 id="course-learning-objectives"><a class="header" href="#course-learning-objectives">Course Learning Objectives</a></h2>
<p>For your convenience, we repeat the course learning objectives.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<h2 id="criteria"><a class="header" href="#criteria">Criteria</a></h2>
<p>Lab 1 is graded by five criteria:</p>
<ul>
<li><a href="lab1/rubric.html#functionality">Functionality</a></li>
<li><a href="lab1/rubric.html#scalability">Scalability</a></li>
<li><a href="lab1/rubric.html#libraries">Libraries</a></li>
<li><a href="lab1/rubric.html#measurements">Measurements</a></li>
<li><a href="lab1/rubric.html#analysis">Analysis</a></li>
</ul>
<p>We list indicators for specific grades below. Please note these are indicators
only. Under specific unexpected circumstances, TA's may use other indicators,
that are not written down here, to modify the grade. This means that these
indicators should not be used as an exhaustive check-list for the grade, but do
provide a strong recommendation for a specific grade.</p>
<h3 id="functionality"><a class="header" href="#functionality">Functionality</a></h3>
<ul>
<li>Weight: (40%)</li>
<li>Related Learning Objectives: L1, L2, L3</li>
</ul>
<p>The program functions correctly according to the requirements described in the
assignment for at least an &quot;adequate&quot; application.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The program does not compile.</td></tr>
<tr><td></td><td>The program exits with errors not due to the user.</td></tr>
<tr><td></td><td>The program does not adhere to all requirements of an &quot;adequate application&quot;.</td></tr>
<tr><td>6 (adequate)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors, unless the user does something wrong.</td></tr>
<tr><td></td><td>The program adheres to all requirements of an &quot;adequate application&quot;.</td></tr>
<tr><td>8 (good)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The program adheres to all requirements of an &quot;good application&quot;.</td></tr>
<tr><td>10 (excellent)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The program adheres to all requirements of an &quot;excellent application&quot;.</td></tr>
</tbody></table>
<h3 id="scalability"><a class="header" href="#scalability">Scalability</a></h3>
<ul>
<li>Weight: (20%)</li>
<li>Related Learning Objectives: L1, L2</li>
</ul>
<p>The program is constructed in such a way that in the case multiple computational
nodes (e.g. more Spark workers) work on the problem concurrently, there is
potential for the performance of the program to increase.</p>
<p>An example when this is not the case is where a <code>map()</code> is not applied to a
distributed dataset (e.g. an <code>RDD</code> or a <code>DataSet</code>), but perhaps by mistake the
student has first used <code>dataset.collect()</code> (taking the data out of the Spark
context) follow by a plain Scala <code>.map()</code>. The map is now applied on a locally
collected dataset, preventing other nodes from performing useful work.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>More than one step is implemented in a non-scalable fashion, where it could have been implemented in a scalable fashion.</td></tr>
<tr><td>6 (adequate)</td><td>There is at most one step implemented in a non-scalable fashion, where it could have been implemented in a scalable fashion.</td></tr>
<tr><td>8 (good)</td><td>All steps are implemented in a scalable fashion where applicable.</td></tr>
<tr><td>10 (excellent)</td><td>As with (good), in addition to comments in the code describing for each step that causes a shuffle, that it does so and why.</td></tr>
</tbody></table>
<h3 id="libraries"><a class="header" href="#libraries">Libraries</a></h3>
<ul>
<li>Weight: (10%)</li>
<li>Related Learning objectives: L1, L3</li>
</ul>
<p>The program is constructed by using Spark SQL.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student has not used Spark SQL to implement the specified functionality.</td></tr>
<tr><td>6 (adequate)</td><td>The student has used Spark SQL to implement the specified functionality.</td></tr>
<tr><td>8 (good)</td><td>The student has used Spark SQL and does not construct compute paths by combining more primitive Spark functions into functions that already exist in the Spark (SQL) API.</td></tr>
<tr><td>10 (excellent)</td><td>The student has introduced abstractions for re-usability. E.g. they have developed a library on top of the Spark libraries providing an easier-to-use API to make similar queries.</td></tr>
</tbody></table>
<h3 id="measurements"><a class="header" href="#measurements">Measurements</a></h3>
<ul>
<li>Weight: (5%)</li>
<li>Related Learning Objectives: L5</li>
</ul>
<p>The run-time of various computational steps of the program are profiled (this
functionality exists in Spark). The run-time of the specified computational
steps is reported.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student has not reported any measurements.</td></tr>
<tr><td>10 (excellent)</td><td>The student has reported the run-time of the specified computational steps exposed by Spark's profiling functionality.</td></tr>
</tbody></table>
<h3 id="analysis"><a class="header" href="#analysis">Analysis</a></h3>
<ul>
<li>Weight: (25%)</li>
<li>Related Learning Objectives: L1, L2, L3, L5</li>
</ul>
<p>The student shows a thorough understanding of the produced code and its behavior
in the context of Apache Spark. This is conveyed through the code, comments in
the code, and the report.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student shows a lack of understanding of the constructs used in the solution.</td></tr>
<tr><td></td><td>The code does not contain any descriptive comments.</td></tr>
<tr><td></td><td>There report provided cannot be considered a reasonable attempt.</td></tr>
<tr><td>6 (adequate)</td><td>The student shows a decent understanding of the constructs used in their solution, but often makes minor mistakes.</td></tr>
<tr><td></td><td>The student has explained most non-trivial constructs in their solution.</td></tr>
<tr><td></td><td>The student explains the measurement results, but gives no hypotheses for anomalies.</td></tr>
<tr><td>8 (good)</td><td>The student shows a decent understanding of the constructs used in their solution, and only makes minor to negligible mistakes in their analysis.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
<tr><td></td><td>The student explains all aspects of the measurement results, and gives acceptable hypotheses for anomalies.</td></tr>
<tr><td>10 (excellent)</td><td>The student shows a thorough understanding of the constructs used in their solution, without making any mistakes.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
<tr><td></td><td>The student explains all aspects of the measurement results, and gives a correct thorough explanation for anomalies.</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-2"><a class="header" href="#lab-2">Lab 2</a></h1>
<p><code>Workload: approx. 39 hours per student</code></p>
<p>This lab assignment is about large-scala data processing using Spark and Amazon
EMR.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="before-you-start-1"><a class="header" href="#before-you-start-1">Before you start</a></h2>
<h3 id="aws-accounts"><a class="header" href="#aws-accounts">AWS Accounts</a></h3>
<p>Each member of your group as signed up in Brightspace has received an e-mail
with a username (group-XX), a password, and an AWS console login link. This
account can be used to perform the lab.</p>
<p>Due to some issues with AWS Educate, we will not use individual AWS Educate
accounts, but multiple accounts linked to a single root account. This root
account has <strong>a shared and limited pool of credits for all students</strong>. Each
group can absorb about 50 USD from the pool.</p>
<p>Because we are using a shared and limited resource, be aware of the following
rules:</p>
<h3 id="terminate-your-cluster-after-use"><a class="header" href="#terminate-your-cluster-after-use">Terminate your cluster after use.</a></h3>
<ul>
<li>You pay for cluster per commissioned minute. After you are done working
with a cluster, you <em><strong>MUST terminate</strong></em> the cluster, to avoid unnecessary
costs.</li>
</ul>
<h3 id="run-the-cluster-in-the-correct-region"><a class="header" href="#run-the-cluster-in-the-correct-region">Run the cluster in the correct region.</a></h3>
<ul>
<li>Please run clusters in regions europe (like <code>eu-west-1</code> or <code>eu-central-1</code>). If you run into spot instance limits, it might be beneficial to change the region you are using. Note: this is different than what was here before, that was for previous years. </li>
</ul>
<h3 id="after-changing-your-code-take-small-steps-before-spawning-a-large-cluster"><a class="header" href="#after-changing-your-code-take-small-steps-before-spawning-a-large-cluster">After changing your code, take small steps before spawning a large cluster.</a></h3>
<ul>
<li>First, make sure your application runs correctly for a tiny data set on
your laptop (e.g. in the Docker containers of Lab 1). Check the result and
continue if it's as expected.</li>
<li>Then, you can move to AWS and use a tiny cluster on a slightly larger data
set. Check the result and continue if it's as expected.</li>
<li>Then, you can increase the cluster resources and try again on a larger
data set. Check the result and continue if it's as expected.</li>
<li>Continue in this way until your goal.</li>
</ul>
<h3 id="your-cluster-must-be-called-after-your-group-as-follows-group-xx"><a class="header" href="#your-cluster-must-be-called-after-your-group-as-follows-group-xx">Your cluster must be called after your group, as follows: <code>group-xx</code></a></h3>
<ul>
<li>Failure to name your cluster after your group could result in random 
terminations of your cluster.</li>
</ul>
<h3 id="do-not-interfere-with-clusters-of-other-groups"><a class="header" href="#do-not-interfere-with-clusters-of-other-groups">Do not interfere with clusters of other groups:</a></h3>
<ul>
<li>Because the setup this year is a work-around, it is not a watertight setup.
It may be that you discover ways to interfere with other groups. All activity 
for your AWS account is logged. If you interfere with other groups, you will 
immediately fail this course. Be careful what actions you perform, and only
perform them if you are absolutely sure it will not disturb the general 
progress of the lab for others.</li>
</ul>
<p>Make sure you have read the introduction on Amazon Web services in the guide
chapter before starting the assignment.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="assignment-1"><a class="header" href="#assignment-1">Assignment</a></h2>
<p>Your task is to run your Spark application on the entire <a href="https://registry.opendata.aws/osm/">OpenStreetMap data
set</a>. The <a href="https://open.quiltdata.com/b/osm-pds">entire planet</a> is available on Amazon S3. Processing the entire 
planet requires a significant amount of resources, which is why an iterative 
process of improving your application and running it on increasing input data 
set sizes is required.</p>
<p>Start by downloading a bigger country from <a href="https://download.geofabrik.de/europe/">Geofabrik</a> and convert it to an ORC 
file locally. Before running your application on Amazon EMR you should run your 
application locally on a bigger data set. <strong>This helps you catch performance 
and scalability issues early-on and prevents you from wasting AWS credits</strong>.</p>
<p>Use the Spark <a href="https://spark.apache.org/docs/3.1.2/tuning.html">tuning guide</a> and the <a href="https://spark.apache.org/docs/3.1.2/sql-performance-tuning.html">SQL performance tuning page</a> to
understand how you can improve the scalability and performance of your
application.</p>
<h3 id="running-your-application-on-amazon-emr"><a class="header" href="#running-your-application-on-amazon-emr">Running your application on Amazon EMR</a></h3>
<p>Please review the information presented 
<a href="lab2/../getting-started/amazon-web-services.html">in the getting started guide</a>, where
it is explained how to spawn your own cluster and how to upload your application
to it.</p>
<p>To run Spark on multiple nodes we are using EMR for this assignment. Selecting
suitable instance types and cluster configurations to efficiently map resource
requirements from your application is an important part of this assignment. Next
to modifying your application, understanding resource utilization and using
different cluster configurations are part of the iterative process to
efficiently scale your application.</p>
<p>When you feel confident your application is ready to process a bigger input data
set (the Netherlands should run in minutes on a modern laptop), you can package
your application for execution on EMR. The <a href="https://github.com/sbt/sbt-assembly">sbt-assembly</a> plugin (to build fat 
JARs - that include dependencies you can use to optimize your implementation) is
available in your project so run the <code>assembly</code> command to package your 
application.</p>
<p>Please note:</p>
<ul>
<li>
<p>Always <strong>start with a small number of small instance types</strong> e.g. 1 master
node  (<code>c5.xlarge</code>) and 5 core nodes (<code>c5.xlarge</code>). Make sure your application
is scalable before spinning up large clusters (and/or more expensive instance
types) to prevent wasting credits.</p>
</li>
<li>
<p>Check the following links for information about configuring Spark on EMR:</p>
<ul>
<li>https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-configure.html</li>
<li>https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/</li>
</ul>
</li>
<li>
<p>You don't have to specify a master in your Spark Session builder or in the
arguments of spark-submit.</p>
</li>
<li>
<p>Write the resulting ORC file to your S3 bucket.
(<code>s3://&lt;your-bucket&gt;/output.orc</code>)</p>
</li>
<li>
<p>Scalability comes at a cost, you can't ignore a proper trade-off between
runtime and cost. For example, decreasing the run time by 10% while increasing
the monetary cost by 500% is typically not acceptable.</p>
</li>
</ul>
<h3 id="data-sets-on-s3"><a class="header" href="#data-sets-on-s3">Data sets on S3</a></h3>
<p>The following data sets of increasing size are available on S3 and can be used
in your iterative development process:</p>
<h4 id="openstreetmap-2"><a class="header" href="#openstreetmap-2">OpenStreetMap:</a></h4>
<ol>
<li>France (1.2 GB) - <code>s3://abs-tudelft-sbd-2022/france.orc</code></li>
<li>United States (8.8 GB) - <code>s3://abs-tudelft-sbd-2022/north-america.orc</code></li>
<li>Europe (27.7 GB) - <code>s3://abs-tudelft-sbd-2022/europe.orc</code></li>
<li>Planet (75.8 GB) - <code>s3://osm-pds/planet/planet-latest.orc</code></li>
</ol>
<h4 id="alos"><a class="header" href="#alos">ALOS:</a></h4>
<ul>
<li>Parquet files are in: <code>s3://abs-tudelft-sbd-2022/ALPSMLC30.parquet/</code></li>
<li>The Parquet files contain statistics that help reduce the amount of time it
takes to load a file. For example, for a so-called row-group, it stores the
minimum and maximum values of numeric columns. If you apply a latitude and
longitude range filter on the ALOS data set immediately after loading, Spark
will push down this filter to the Parquet reader. This causes only row groups
within a certain latitude or longitude range to be loaded from storage, rather
than loading all ALOS points for the whole world. <strong>This will save you a
significant amount of time.</strong></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="deliverables-1"><a class="header" href="#deliverables-1">Deliverables</a></h2>
<p>The deliverable for this lab are a report documenting:</p>
<ul>
<li>The iterative decision-making process</li>
<li>Improvements made to the application and their effects</li>
</ul>
<p>Add the source of your (optimized) application in the repository. You can copy
the sources from your lab 1 repository as a starting point.</p>
<p>Write the report in the <code>README.md</code> markdown file in the root of your
repository. Use the template as a starting point for your report.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rubric-for-lab-2"><a class="header" href="#rubric-for-lab-2">Rubric for Lab 2</a></h1>
<h2 id="course-learning-objectives-1"><a class="header" href="#course-learning-objectives-1">Course Learning Objectives</a></h2>
<p>For your convenience, we repeat the course learning objectives.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<h2 id="criteria-1"><a class="header" href="#criteria-1">Criteria</a></h2>
<p>Lab 2 is graded by four criteria:</p>
<ul>
<li><a href="lab2/rubric.html#approach">Approach</a></li>
<li><a href="lab2/rubric.html#application">Application</a></li>
<li><a href="lab2/rubric.html#cluster">Cluster</a></li>
</ul>
<p>The grade for this lab is expressed as:</p>
<pre><code>Grade = Approach + Application + Cluster
</code></pre>
<p>We list indicators for specific grades below. Please note these are indicators
only. Under specific unexpected circumstances, TA's may use other indicators,
that are not written down here, to modify the grade. This means that these
indicators should not be used as an exhaustive check-list for the grade, but do
provide a strong recommendation for a specific grade.</p>
<h3 id="approach"><a class="header" href="#approach">Approach</a></h3>
<ul>
<li>Weight: 50%</li>
<li>Related Learning Objectives: L1</li>
</ul>
<p>Because the development iterations of big data applications can be large in both
cost and time, the student makes careful, well-informed decisions before
executing the next iteration, and documents their decision-making approach.</p>
<p>Significant iterations that are to be documented include:</p>
<ol>
<li>The first succesful run on the first data set.</li>
<li>After significant optimizations are performed and a full run is completed on
any of the data sets.</li>
<li>When going from a smaller dataset to a larger data set results in
significant changes in where bottlenecks are in the application.</li>
</ol>
<p>Examples:</p>
<ul>
<li>As described in the previous iteration, we discovered a new bottleneck X. This
was mitigated. After re-running the application, a next bottleneck occurs in
Y. We have thought of method A, B and C to mitigate this, and have ultimately
chosen B because of reason Z.</li>
<li>The query was initially performed on the &quot;The Netherlands&quot; data set, and was
now run on the USA data set. The USA has significantly more mountain ranges
so bottleneck X caused by operation Y was relatively increased. We
have therefore thought of method A, B and C to mitigate this, and have
ultimately chosen B because of reason Z.</li>
</ul>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student does not explain why they have taken a specific approach before executing a development iteration.</td></tr>
<tr><td></td><td>There is no explicit reasoning behind a specific approach to a next development iteration.</td></tr>
<tr><td>6 (adequate)</td><td>The student describes their reasoning behind a specific approach to a next development iteration, but their description contains minor mistakes or makes a limited amount of incorrect assumptions.</td></tr>
<tr><td>8 (good)</td><td>The student describes their reasoning behind a specific approach to a next development iteration.</td></tr>
<tr><td>10 (excellent)</td><td>The student describes their reasoning behind multiple (where applicable) alternative approaches (and their selection) to a next development iteration.</td></tr>
</tbody></table>
<h3 id="application"><a class="header" href="#application">Application</a></h3>
<ul>
<li>Weight: 30%</li>
<li>Related Learning Objectives: L1, L2, L3, L5</li>
</ul>
<p>The student selects and executes an appropriate improvement at the application
implementation level to overcome bottlenecks.</p>
<p>An examples where an application implementation improvement is appropriate:</p>
<ul>
<li>A dataset is reused multiple times and fits in worker memory, so the student
has applied <code>cache()</code>.</li>
</ul>
<p>An example where an application implementation is not appropriate:</p>
<ul>
<li>A dataset is <code>repartition()</code>ed based on some arbitrary value that only works
well in a specific case (e.g. for a specific dataset), but does not work well
for other case (e.g. another dataset). Such values should be derived
dynamically.</li>
</ul>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student applies several inappropriate strategies for application improvements.</td></tr>
<tr><td>10 (excellent)</td><td>The student applies appropriate strategies for application improvement.</td></tr>
</tbody></table>
<h3 id="cluster"><a class="header" href="#cluster">Cluster</a></h3>
<ul>
<li>Weight: 20%</li>
<li>Related Learning Objectives: L1, L4</li>
</ul>
<p>The student selects and executes an appropriate strategy at the cluster level to
overcome bottlenecks in their implementation, and takes into consideration the
trade-off between cost and performance.</p>
<p>An example where this is done appropriately is: if the application throughput is
bound by network I/O, the student can choose to run the application on instances
that have more network bandwidth.</p>
<p>An example where this is not done appropriately is: the performance of the
application is bound by memory size, but to mitigate this the student moves to
instance types with GPUs to obtain more compute, but not more memory, or the
other way around.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student applies several inappropriate strategies for cluster improvements.</td></tr>
<tr><td>10 (excellent)</td><td>The student applies appropriate strategies for cluster improvement.</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lab-3"><a class="header" href="#lab-3">Lab 3</a></h1>
<p><code>Workload: approx. 26 hours per student</code></p>
<p>In the third and final lab of SBD we will be implementing a <em><strong>streaming</strong></em>
application.</p>
<p>Apache Spark, that we've used in the first two labs, is quite good at processing
large batches of data. However, there is another class of big data applications
that gather and transform many tiny items coming from many so called <em>producers</em>
, transform the information, and send it off to many so called <em>consumers</em>,
typically with a relatively low latency. Such applications are called
<em>streaming</em> applications.</p>
<p>One example is when you're using a navigation service when driving a car. You're
producing location information about your car, this is streamed to a service
that collects this information from many other people as well. In turn it can
detect traffic jams. It gathers, transforms, and finally outputs traffic jam
information back to your phone, all in real-time.</p>
<p>Since Apache Spark is more geared towards doing batch processing, for this lab
we will be using <em>Apache Kafka</em>; a well-known framework used a lot to create
such streaming applications in scalable fashion.</p>
<p>In typical streaming applications, we stream data into a pipeline that processes
the data by filtering and transforming it towards a desired result. At the end,
there is usually some form of visualization. Such a pipeline, that we will use
for this lab, is depicted in the figure below:</p>
<p><img src="lab3/../assets/images/kafka_pipeline.png" alt="Streaming pipeline" /></p>
<p>Your assignment will be to use Apache Kafka to transform a stream of raw data
into something more usable for our goal. Before explaining the exact goal, we
will first briefly go over the various components of the pipeline shown in the
figure.</p>
<h3 id="kafka-stream"><a class="header" href="#kafka-stream">Kafka Stream</a></h3>
<p>Kafka knows the concept of <em><strong>streams</strong></em> that have a specific <em><strong>topic</strong></em>.
<em><strong>Producers</strong></em> and <em><strong>consumers</strong></em> can <em><strong>subscribe</strong></em> to topics. Through the
APIs of Kafka, we can pull and push data from and to such Kafka streams.</p>
<h3 id="producer"><a class="header" href="#producer">Producer</a></h3>
<p>The producer, contained in the <code>Producer</code> project creates (for the purpose of
this lab) randomly generated data records and outputs them on a Kafka stream
with the topic <code>events</code>.</p>
<p>For typical applications, this is some source like IOT devices, people's phones,
social media outputs, etc. Many services have streaming APIs, like e.g.
<a href="https://developer.twitter.com/en/docs/labs/sampled-stream/api-reference/get-tweets-stream-sample">Twitter</a>, but for this lab, we just <em>mimic</em> a stream-producing service, since
that provides a more learning-friendly environment.</p>
<p>The Producer will be provided, you don't have to do anything on the Producer.</p>
<h3 id="transformer"><a class="header" href="#transformer">Transformer</a></h3>
<p>The transformer consumes records from the Kafka stream of the topic <code>events</code>. It
produces a Kafka stream of the topic <code>updates</code>. It therefore transforms 'events'
to 'updates', where usually many events are aggregated into some statistic, or
filtered by some predicate.</p>
<p>Why this intermediate step? Why can't a consumer process the <code>events</code> itself?
For typical applications, this is usually not done by the consumer of the data
because the producers send so much data on the <code>events</code> topic, that e.g. a
simple phone or a laptop could not process it --- only a scalable application
run on a cluster could process so much information so quickly. This is the type
of framework that Kafka provides.</p>
<p>The Transformer is the component that you will implement. By doing so you will
learn to use a streaming framework like Kafka to write scalable real-time
streaming applications.</p>
<h3 id="consumer"><a class="header" href="#consumer">Consumer</a></h3>
<p>The consumer finally acts as a <em>sink</em>, and will process the incoming updates
from the <code>updates</code> topic. It will do some final pre-processing before sending
the updates to the visualizer.</p>
<p>The Consumer will be provided, you don't have to do anything on the Consumer.</p>
<h3 id="visualizer"><a class="header" href="#visualizer">Visualizer</a></h3>
<p>The visualizer acts as a webserver, providing a webpage where you can view your
updates in a real-time fashion. <em><strong>Once it is up and running</strong></em> (see next page),
you can browse to it by using this link: <a href="http://localhost:1234">localhost:1234</a>.</p>
<h3 id="further-reading-on-kafka"><a class="header" href="#further-reading-on-kafka">Further reading on Kafka</a></h3>
<p>It is required that you read the below references to be able to understand how 
to get started with Kafka.</p>
<ul>
<li><a href="https://kafka.apache.org/intro">Kafka Intro</a></li>
<li><a href="https://kafka.apache.org/26/documentation/streams/architecture.html">Kafka Streams Architecture</a></li>
<li><a href="https://kafka.apache.org/26/documentation/streams/developer-guide/dsl-api.html">Kafka DSL API</a></li>
<li><a href="https://kafka.apache.org/26/documentation/streams/developer-guide/processor-api.html">Kafka Processor API</a></li>
<li><a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/kstream/KStream.html">Kafka KStream API docs</a></li>
<li><a href="https://github.com/confluentinc/kafka-streams-examples">Kafka Stream Examples</a></li>
<li><a href="https://github.com/azhur/kafka-serde-scala">JSON Serde support for Kafka</a></li>
<li><a href="https://jaceklaskowski.gitbooks.io/mastering-kafka-streams/">Mastering Kafka Streams</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="before-you-start-2"><a class="header" href="#before-you-start-2">Before you start</a></h2>
<p>For this assignment a <code>docker-compose.yml</code> file is provided to easily build and
run all required services in Docker containers.
<a href="https://docs.docker.com/compose/">Docker Compose</a> is a tool for defining and
running multi-container Docker applications. Please
<a href="https://docs.docker.com/compose/install/">install</a> the latest version for
this assignment. Also take a look at the
<a href="https://docs.docker.com/compose/compose-file/">Compose file reference</a>.</p>
<p>For this assignment the following containers are defined in the
<code>docker-compose.yml</code> file:</p>
<table><thead><tr><th>Container</th><th>Description</th></tr></thead><tbody>
<tr><td><code>zookeeper-server</code></td><td>A Zookeeper server instance. Kafka requires Zookeeper to run.</td></tr>
<tr><td><code>kafka-server</code></td><td>A single Kafka server instance.</td></tr>
<tr><td><code>producer</code></td><td>A Kafka producer running the Producer application.</td></tr>
<tr><td><code>transformer</code></td><td>A Kafka stream processor running the Transformer application.</td></tr>
<tr><td><code>consumer</code></td><td>A Kafka consumer running the Consumer application and the visualizer. The visualizer can be accessed when the service is running by navigating to <a href="http://localhost:1234">localhost:1234</a>.</td></tr>
<tr><td><code>events</code></td><td>A Kafka consumer subscribed to the <code>events</code> topic, writing records to the console. Useful for debugging.</td></tr>
<tr><td><code>updates</code></td><td>A Kafka consumer subscribed to the <code>updates</code> topic, writing records to the console. Useful for debugging.</td></tr>
</tbody></table>
<p>To start the containers, navigate to the repository root directory and run
<code>docker-compose up</code>. This will build and start all the containers defined in
the compose file. To start over, stop and remove everything with
<code>docker-compose down</code>.</p>
<p>The Transformer application, which you will develop for this assignment, is
built and run in the <code>transformer</code> service container. To start an interactive
<code>sbt</code> shell use <code>docker-compose exec transformer sbt</code> from the repository root.
From there, you can <code>compile</code> and <code>run</code> the Transformer application. Make sure
the other services are up before starting your streams application. You can stop
your application before running it again after changing the source using
<code>CTRL+C</code>.</p>
<p>You can use the following <code>docker-compose</code> commands to interact with your
running containers:</p>
<table><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody>
<tr><td><code>docker-compose up</code></td><td>Create and start containers.</td></tr>
<tr><td><code>docker-compose up -d</code></td><td>Create and start containers in the background.</td></tr>
<tr><td><code>docker-compose down</code></td><td>Stop and remove containers, networks, images, and volumes.</td></tr>
<tr><td><code>docker-compose start</code></td><td>Start services.</td></tr>
<tr><td><code>docker-compose restart</code></td><td>Restart services.</td></tr>
<tr><td><code>docker-compose stop</code></td><td>Stop services.</td></tr>
<tr><td><code>docker-compose rm</code></td><td>Remove stopped containers.</td></tr>
<tr><td><code>docker-compose logs --follow &lt;SERVICE&gt;</code></td><td>View and follow log output from containers.</td></tr>
</tbody></table>
<p>For a full list of available commands please refer to the
<a href="https://docs.docker.com/compose/reference/overview/">CLI Reference</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="assignment-2"><a class="header" href="#assignment-2">Assignment</a></h2>
<p>The scenario of the first lab has worsened so much that the whole world is about
to get flooded. The people of Waterworld have united under the banner of the
United Mariners (UM) and coordinate their efforts through a number of massive 
ships (called UM motherships), temporarily sheltering those who seek refuge in 
Waterworld from the dangers of the ocean.</p>
<p>To coordinate the UM effort, they have released an app for refugees that have
satellite phones. The app can send Waterworld entry events to notify a number of
refugees from some city have boarded some vessel and are now seeking refuge in
one of the motherships.</p>
<p>The Waterworld entry events enter the application via the <code>events</code> topic input
stream and are formatted as <a href="https://en.wikipedia.org/wiki/JSON">JSON documents</a>, and have the following schema:</p>
<table><thead><tr><th>Field</th><th>JSON Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>timestamp</code></td><td>number</td><td>The Unix timestamp (non-leap-milliseconds since January 1, 1970 UTC) of the check-in.</td></tr>
<tr><td><code>city_id</code></td><td>number</td><td>A unique ID of the city where the check-in was made.</td></tr>
<tr><td><code>city_name</code></td><td>string</td><td>The name of the city where the check-in was made.</td></tr>
<tr><td><code>refugees</code></td><td>number</td><td>The number of refugees entering the vessel.</td></tr>
</tbody></table>
<p><em>Disclaimer:</em> While technically not required, we include the city name to
facilitate debugging. This goes for some requirements below as well.</p>
<p>The requirements of the Kafka application that you must implement based on this
description are described below in three tiers (&quot;adequate&quot;, &quot;good&quot; and &quot;
excellent&quot;). These tiers correspond to one of the grading criteria (explained on
the rubric page). To pass the lab, you must at least achieve the &quot;adequate&quot;
tier. &quot;Good&quot; and &quot;excellent&quot; tier applications contain <strong>additional</strong>
requirements (and may supersede previous requirements where applicable) that the
application needs to adhere to.</p>
<h3 id="adequate-application-requirements-1"><a class="header" href="#adequate-application-requirements-1">&quot;Adequate&quot; application requirements</a></h3>
<ol>
<li>
<p>The application outputs a Kafka stream on the topic <code>updates</code> with the total
number of refugees in Waterworld, whenever an incoming entry event is pulled
from the <code>events</code> input stream. The <code>update</code> data is serialized as a JSON
document with the following schema:</p>
<table><thead><tr><th>Field</th><th>JSON Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>refugees</code></td><td>number</td><td>The total number of refugees.</td></tr>
</tbody></table>
</li>
</ol>
<ul>
<li>Example:</li>
</ul>
<p>On the input stream, the following events are received:</p>
<pre><code class="language-json">{&quot;timestamp&quot;:1, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;refugees&quot;:10}
{&quot;timestamp&quot;:3, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;refugees&quot;:20}
{&quot;timestamp&quot;:4, &quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:30}
</code></pre>
<p>On the output stream, the following updates are required:</p>
<pre><code class="language-json">{&quot;refugees&quot;:10}
{&quot;refugees&quot;:30}
{&quot;refugees&quot;:60}
</code></pre>
<h3 id="good-application-requirements-1"><a class="header" href="#good-application-requirements-1">&quot;Good&quot; application requirements</a></h3>
<ol start="2">
<li>
<p>Superseding requirement 1, the application outputs the total number of
refugees in Waterworld <em><strong>for each city</strong></em>. The <code>update</code> data is serialized
as a JSON document with the following schema:</p>
<table><thead><tr><th>Field</th><th>JSON Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>city_id</code></td><td>number</td><td>The total number of refugees.</td></tr>
<tr><td><code>city_name</code></td><td>string</td><td>The name of the city.</td></tr>
<tr><td><code>refugees</code></td><td>number</td><td>The total number of refugees in the respecitve city.</td></tr>
</tbody></table>
</li>
</ol>
<ul>
<li>Example:</li>
</ul>
<p>On the input stream, the following events are received:</p>
<pre><code class="language-json">{&quot;timestamp&quot;:1, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;refugees&quot;:10}
{&quot;timestamp&quot;:3, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;refugees&quot;:20}
{&quot;timestamp&quot;:4, &quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:30}
</code></pre>
<p>On the output stream, the following updates are required:</p>
<pre><code class="language-json">{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;refugees&quot;:10}
{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;, &quot;refugees&quot;:30}
{&quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:30}
</code></pre>
<h3 id="excellent-application-requirements-1"><a class="header" href="#excellent-application-requirements-1">&quot;Excellent&quot; application requirements</a></h3>
<ol start="3">
<li>
<p>The applications keeps track of the events within a window of N seconds, and,
superseding requirement 2, also sends out the change in refugee count within 
that window.</p>
<table><thead><tr><th>Field</th><th>JSON Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>city_id</code></td><td>number</td><td>The total number of refugees.</td></tr>
<tr><td><code>city_name</code></td><td>string</td><td>The name of the city.</td></tr>
<tr><td><code>refugees</code></td><td>number</td><td>The total number of refugees in the respective city.</td></tr>
<tr><td><code>change</code></td><td>number</td><td>The total number of refugees entering Waterworld from the respective city in the last N seconds.</td></tr>
</tbody></table>
</li>
</ol>
<ul>
<li>Example:</li>
</ul>
<p>On the input stream, the following events are received:</p>
<pre><code class="language-json">{&quot;timestamp&quot;:1, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:10}
{&quot;timestamp&quot;:2, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:20}
{&quot;timestamp&quot;:3, &quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:30}
{&quot;timestamp&quot;:4, &quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:40}
{&quot;timestamp&quot;:5, &quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:12}
</code></pre>
<p>When N = 2, on the output stream, the following updates are required:</p>
<pre><code class="language-json">{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:10, &quot;change&quot;: 10}  // At timestamp 1
{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:30, &quot;change&quot;: 30}  // At timestamp 2
{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:30, &quot;change&quot;: 20}  // At timestamp 3
{&quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:30, &quot;change&quot;: 30}  // At timestamp 3
{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:70, &quot;change&quot;: 40}  // At timestamp 4
{&quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:42, &quot;change&quot;: 12}  // At timestamp 5
{&quot;city_id&quot;:1, &quot;city_name&quot;:&quot;Delft&quot;,     &quot;refugees&quot;:70, &quot;change&quot;:  0}  // At timestamp 6
{&quot;city_id&quot;:2, &quot;city_name&quot;:&quot;Rotterdam&quot;, &quot;refugees&quot;:42, &quot;change&quot;:  0}  // At timestamp 7
</code></pre>
<h3 id="supplying-the-time-window"><a class="header" href="#supplying-the-time-window">Supplying the time window</a></h3>
<p>The time window will be supplied on the command-line as the first argument
representing the time window size in seconds.</p>
<h3 id="general-hints-and-recommended-approach"><a class="header" href="#general-hints-and-recommended-approach">General hints and recommended approach</a></h3>
<ul>
<li>Streaming frameworks like Kafka usually work with very strong notions of
<strong>stateless</strong> and <strong>stateful</strong> operations.
<ul>
<li>An example of a <strong>stateless</strong> operation is to filter records of a stream
based on their content.</li>
<li>An example of a <strong>stateful</strong> operation is to update some existing data
structure based on streamed records.</li>
</ul>
</li>
<li>Keeping track of total number of evacuees (or that have been included in your
current window of interest) is <strong>stateful</strong>, and can be done in an abstraction
called a <strong>state store</strong>.
<ul>
<li>You can operate on state stores whenever a new record arrives using so
called <strong>stateful transformations</strong> of records.</li>
<li>It is recommended to use the <a href="https://kafka.apache.org/26/documentation/streams/developer-guide/processor-api.html">Processor API</a> for this.
Check <a href="https://kafka.apache.org/26/documentation/streams/developer-guide/dsl-api.html#applying-processors-and-transformers-processor-api-integration">this documentation</a> on how to apply processors and transformers.
Consider the differences between <a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/kstream/KStream.html#process-org.apache.kafka.streams.processor.ProcessorSupplier-java.lang.String...-">processors</a> and <a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/kstream/KStream.html#transform-org.apache.kafka.streams.kstream.TransformerSupplier-java.lang.String...-">transformers</a> and pick
the one that best suits this application.</li>
<li>While it is technically possible to use Kafka's Windowing abstractions, it
is <strong>not recommended</strong>, because for requirement 3 it does not exactly
match our use-case of sending the change &quot;0&quot; update.</li>
</ul>
</li>
<li>What is slightly similar to building up DAGs in Spark is what in Kafka is
called building up the stream Topology.
<ul>
<li>This is also lazily evaluated and only starts doing its thing when you
call
<code>.start()</code> on a <code>KafkaStreams</code>.</li>
<li>You can obtain a Topology description for debugging after using e.g.
<code>val topology = builder.build()</code> and then <code>println(topology.describe())</code>.</li>
<li>If you copy-paste the description <a href="https://zz85.github.io/kafka-streams-viz/">in this tool</a>, you can visualize it.</li>
</ul>
</li>
<li>You want to convert the JSONs to a Scala case class to be able to process the
data in a type-safe manner. It is recommended to use one of the options from
<a href="https://github.com/azhur/kafka-serde-scala">this repository</a>. <code>circe</code> is a good option that worked for the TA's.</li>
<li>An example of how to implement a transformer in scala can be found <a href="https://github.com/apache/kafka/blob/trunk/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/TopologyTest.scala">here</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="deliverables-2"><a class="header" href="#deliverables-2">Deliverables</a></h2>
<p>The deliverables for this lab are:</p>
<ol>
<li>Transformer application</li>
<li>Report</li>
</ol>
<p>Deliverables should be contained in your project repository.</p>
<h3 id="kafka-based-application"><a class="header" href="#kafka-based-application">Kafka-based application</a></h3>
<p>All Scala source files required to build your Kafka-based application should be
committed to your Lab 3 repository before the deadline.</p>
<h3 id="report-1"><a class="header" href="#report-1">Report</a></h3>
<p>The results and analysis in the <code>README.md</code> markdown file in the root of your
repository.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rubric-for-lab-3"><a class="header" href="#rubric-for-lab-3">Rubric for Lab 3</a></h1>
<h2 id="course-learning-objectives-2"><a class="header" href="#course-learning-objectives-2">Course Learning Objectives</a></h2>
<p>For your convenience, we repeat the course learning objectives.</p>
<p>By the end of this course, you will be able to:</p>
<table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody>
<tr><td>L1</td><td>Use basic big data processing systems like Hadoop and MapReduce.</td></tr>
<tr><td>L2</td><td>Implement parallel algorithms using the in-memory Spark framework, and streaming using Kafka.</td></tr>
<tr><td>L3</td><td>Use libraries to simplify implementing more complex algorithms.</td></tr>
<tr><td>L4</td><td>Identify the relevant characteristics of a given computational platform to solve big data problems.</td></tr>
<tr><td>L5</td><td>Utilize knowledge of hardware and software tools to produce an efficient implementation of the application.</td></tr>
</tbody></table>
<h2 id="criteria-2"><a class="header" href="#criteria-2">Criteria</a></h2>
<p>Lab 3 is graded by the following criteria:</p>
<ul>
<li><a href="lab3/rubric.html#functionality">Functionality</a></li>
<li><a href="lab3/rubric.html#streaming">Streaming</a></li>
<li><a href="lab3/rubric.html#analysis">Analysis</a></li>
</ul>
<p>The grade for this lab is expressed as:</p>
<pre><code>Grade = Functionality + Streaming + Analysis
</code></pre>
<p>We list indicators for specific grades below. Please note these are indicators
only. Under specific unexpected circumstances, TA's may use other indicators,
that are not written down here, to modify the grade. This means that these
indicators should not be used as an exhaustive check-list for the grade, but do
provide a strong recommendation for a specific grade.</p>
<h3 id="functionality-1"><a class="header" href="#functionality-1">Functionality</a></h3>
<ul>
<li>Weight: 60%</li>
<li>Related Learning Objectives: L2, L5</li>
</ul>
<p>The program functions correctly according to the requirements described in the
assignment for at least an &quot;adequate&quot; application.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The program does not compile.</td></tr>
<tr><td></td><td>The program exits with errors not due to the user.</td></tr>
<tr><td></td><td>The program does not adhere to all requirements of an &quot;adequate application&quot;.</td></tr>
<tr><td>6 (adequate)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors, unless the user does something wrong.</td></tr>
<tr><td></td><td>The program adheres to all requirements of an &quot;adequate application&quot;.</td></tr>
<tr><td>8 (good)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The program adheres to all requirements of an &quot;good application&quot;.</td></tr>
<tr><td>10 (excellent)</td><td>The program compiles.</td></tr>
<tr><td></td><td>The program exits without errors. When the user does something wrong, a descriptive text of how to correct their input is returned.</td></tr>
<tr><td></td><td>The program adheres to all requirements of an &quot;excellent application&quot;.</td></tr>
</tbody></table>
<h3 id="streaming"><a class="header" href="#streaming">Streaming</a></h3>
<ul>
<li>Weight: 20%</li>
<li>Related Learning Objectives: L2, L5</li>
</ul>
<p>The student makes use of Kafka's ability to implement streamable low-latency
applications. To this end, the student avoids micro-batching as much as
possible, and applies stateless processing methods as much as possible. Of
course, there may be processing steps that do require state.</p>
<p>An example of micro-batching is as follows. Consider some input stream with some
integers: <code>{0, 1, 2, 3}</code> and an application that has to produce an output stream
with the squared value of the integers <code>{0, 1, 4, 9}</code>. (Here, assume every value
is a discrete Kafka record).  A student would be applying micro-batching when
they would place the integers in e.g. a Kafka store, and trigger the squaring
operation and producing output records e.g. periodically or e.g. when the amount
of integers reaches some threshold, processing all input records in the store at
once. The student should avoid micro-batching by using stateless
transformations, since this specific functionality does not require state.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0  (fail)</td><td>The transformer application is in more than one place implemented with stateful methods where they are not required or desired.</td></tr>
<tr><td></td><td>Micro-batching is applied in more than one place.</td></tr>
<tr><td>6  (adequate)</td><td>The transformer application is in at most one place implemented with a stateful method where it is not required or desired.</td></tr>
<tr><td></td><td>Micro-batching is applied in at most one place.</td></tr>
<tr><td>10 (excellent)</td><td>The transformer application uses stateless methods as much as possible, avoiding stateful methods where not required or desired.</td></tr>
<tr><td></td><td>The transformer application produces an update for every incoming record without applying micro-batching.</td></tr>
</tbody></table>
<h3 id="analysis-1"><a class="header" href="#analysis-1">Analysis</a></h3>
<ul>
<li>Weight: 20%</li>
<li>Related Learning Objectives: L1, L2, L3, L5</li>
</ul>
<p>The student shows a thorough understanding of the produced code and its behavior
in the context of Apache Kafka. This is conveyed through the code, comments in
the code, and the report.</p>
<table><thead><tr><th>Grade</th><th>Indicators</th></tr></thead><tbody>
<tr><td>0 (fail)</td><td>The student shows a lack of understanding of the constructs used in the solution.</td></tr>
<tr><td></td><td>The code does not contain any descriptive comments.</td></tr>
<tr><td></td><td>There report provided cannot be considered a reasonable attempt.</td></tr>
<tr><td>6 (adequate)</td><td>The student shows a decent understanding of the constructs used in their solution, but often makes minor mistakes.</td></tr>
<tr><td></td><td>The student has explained most non-trivial constructs in their solution.</td></tr>
<tr><td>8 (good)</td><td>The student shows a decent understanding of the constructs used in their solution, and only makes minor to negligible mistakes in their analysis.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
<tr><td>10 (excellent)</td><td>The student shows a thorough understanding of the constructs used in their solution, without making any mistakes.</td></tr>
<tr><td></td><td>The student has explained all non-trivial constructs in their solution.</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="frequently-asked-questions-faq"><a class="header" href="#frequently-asked-questions-faq">Frequently asked questions (FAQ)</a></h1>
<p>(Thanks to everyone who spent the time to post their fixes to common problems on the Brightspace Q&amp;A)</p>
<h3 id="how-do-i-use-the-h3-library"><a class="header" href="#how-do-i-use-the-h3-library">How do I use the H3 library?</a></h3>
<p>You can import it in your scala file like:</p>
<pre><code>import com.uber.h3core.H3Core
import com.uber.h3core.util.GeoCoord
</code></pre>
<p>And then add to library dependencies in your build.sbt:</p>
<pre><code>&quot;com.uber&quot; % &quot;h3&quot; % &quot;3.7.0&quot;
</code></pre>
<p>(If this does not work for you, let us know!)</p>
<h3 id="i-cannot-connect-to-my-cluster-with-ssh"><a class="header" href="#i-cannot-connect-to-my-cluster-with-ssh">I cannot connect to my cluster with SSH</a></h3>
<p>If you have trouble connecting via SSH to your cluster, the issue might be that you are using the private subnet.</p>
<p>During cluster creation, in step 2 under Networking, select one of the public EC2 subnets.</p>
<p>Another issue might be the standard firewall rules for the EC2 clusters. If you are having this problem, you can try the following steps:</p>
<ol>
<li>Go to your cluster overview page and scroll down to &quot;Network and Security&quot;</li>
<li>Click the link below the &quot;Primary Node - EMR managed security group&quot;</li>
<li>Click &quot;Actions&quot; &gt; &quot;Edit inbound rules&quot; (top right)</li>
<li>Find the rule for SSH (port 22) and click the X on the IP address that is currently there.</li>
<li>Click in the search box and select &quot;0.0.0.0/0&quot;. (or enter a more restrictive IP range if you want)</li>
<li>Click &quot;Save rules&quot;</li>
</ol>
<h3 id="do-i-need-to-keep-the-github-repo-of-lab-1-up-to-date-with-the-changes-i-made-for-lab-2"><a class="header" href="#do-i-need-to-keep-the-github-repo-of-lab-1-up-to-date-with-the-changes-i-made-for-lab-2">Do I need to keep the github repo of lab 1 up to date with the changes I made for lab 2?</a></h3>
<p>No, this is not necessary. If you have specific changes that you discuss in lab 1, make sure to specify that the code for that is in the lab 2 repository.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="quiz-example"><a class="header" href="#quiz-example">Quiz example</a></h2>
<p>The below questions are examples for the quiz at the end of the lab.</p>
<p>There is always only one possible answer.</p>
<ol>
<li>
<p>Which Spark script is used to launch applications on a Spark cluster?</p>
<ul>
<li>A. <code>spark_submit</code></li>
<li>B. <code>spark_drive</code></li>
<li>C. <code>spark_assemble</code></li>
<li>D. <code>spark_shell</code></li>
</ul>
</li>
<li>
<p>Given a dataframe with columns: {'a', 'b', 'c'}, which Data(frame/set) API
function can be used to create a dataframe with columns {'a', 'c'}?</p>
<ul>
<li>A. <code>select(...)</code></li>
<li>B. <code>collect(...)</code></li>
<li>C. <code>reduce(...)</code></li>
<li>D. <code>filter(...)</code></li>
</ul>
</li>
<li>
<p>The OpenStreetMap dataset contains the following three distinct elements:</p>
<ul>
<li>A. nodes, ways, relations</li>
<li>B. breweries, cities, tags</li>
<li>C. tags, nodes, cities</li>
<li>D. addr:city, admin levels, boundary</li>
</ul>
</li>
<li>
<p>The reports were implemented in a file called:</p>
<ul>
<li>A. <code>README.md</code></li>
<li>B. <code>REPORT.txt</code></li>
<li>C. <code>LAB_X.md</code></li>
<li>D. <code>build.sbt</code></li>
</ul>
</li>
<li>
<p>The artifact from SBT that is to be sent off to the Spark cluster has the
extension:</p>
<ul>
<li>A. <code>.jar</code></li>
<li>B. <code>.spark</code></li>
<li>C. <code>.sbt</code></li>
<li>D. <code>.scala</code></li>
</ul>
</li>
</ol>
<p>Answers: </p>
<ol>
<li>A</li>
<li>A</li>
<li>A</li>
<li>A</li>
<li>A</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="useful-links"><a class="header" href="#useful-links">Useful links</a></h2>
<p>Below are some links that are useful:</p>
<ul>
<li><a href="https://rogerdudler.github.io/git-guide">Git cheatsheet</a></li>
</ul>
<h2 id="often-used-api-docs"><a class="header" href="#often-used-api-docs">Often-used API docs:</a></h2>
<ul>
<li><a href="https://spark.apache.org/docs/2.4.6/api/scala/index.html#org.apache.spark.package">Spark all APIs</a></li>
<li><a href="https://spark.apache.org/docs/2.4.6/api/scala/index.html#org.apache.spark.sql.Dataset">Spark DataSet API</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
